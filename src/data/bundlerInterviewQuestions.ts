export type BundlerInterviewTopic =
  | 'common-concepts'
  | 'webpack'
  | 'vite'
  | 'rollup'
  | 'esbuild'
  | 'parcel'
  | 'turbopack'
  | 'rspack'
  | 'rolldown'

export interface BundlerInterviewQuestion {
  id: number
  title: string
  difficulty: 'easy' | 'medium' | 'hard'
  topic: BundlerInterviewTopic
  subtopic: string
  answer: string
  codeExample?: string
  followUp: string
  keyTakeaway: string
}

export interface BundlerTopicConfig {
  id: BundlerInterviewTopic
  label: string
  description: string
}

export const bundlerTopics: BundlerTopicConfig[] = [
  {
    id: 'common-concepts',
    label: 'Common Concepts',
    description: 'Tree shaking, code splitting, HMR, module formats, source maps, dependency graphs',
  },
  {
    id: 'webpack',
    label: 'Webpack',
    description: 'Loaders, plugins, Module Federation, chunk splitting, optimization',
  },
  {
    id: 'vite',
    label: 'Vite',
    description: 'ESM dev server, Rollup production builds, plugin API, dependency pre-bundling',
  },
  {
    id: 'rollup',
    label: 'Rollup',
    description: 'ESM-first output, tree shaking, plugin hooks, output formats',
  },
  {
    id: 'esbuild',
    label: 'esbuild',
    description: 'Go-based speed, bundling and minification, JSX handling, plugin API',
  },
  {
    id: 'parcel',
    label: 'Parcel',
    description: 'Zero-config philosophy, asset graph, transformers, scope hoisting',
  },
  {
    id: 'turbopack',
    label: 'Turbopack',
    description: 'Rust-based architecture, incremental computation, Next.js integration',
  },
  {
    id: 'rspack',
    label: 'Rspack',
    description: 'Rust-based Webpack-compatible bundler, loader and plugin compatibility',
  },
  {
    id: 'rolldown',
    label: 'Rolldown',
    description: 'Rust-based Rollup replacement, Vite integration, compatibility goals',
  },
]

export const bundlerTopicMap: Record<BundlerInterviewTopic, BundlerTopicConfig> =
  Object.fromEntries(bundlerTopics.map((t) => [t.id, t])) as Record<BundlerInterviewTopic, BundlerTopicConfig>

export const bundlerInterviewQuestions: BundlerInterviewQuestion[] = [
{
  id: 1,
  title: 'What is a JavaScript bundler and why do we need one?',
  difficulty: 'easy',
  topic: 'common-concepts',
  subtopic: 'bundling-basics',
  answer: 'A JavaScript bundler is a tool that takes multiple JavaScript files and their dependencies, then combines them into one or more optimized output files (bundles) suitable for the browser. Bundlers are needed because browsers historically could not natively handle modular JavaScript with import/export statements, and loading hundreds of individual files over HTTP would be extremely slow. Modern bundlers like Webpack, Rollup, and esbuild also perform optimizations such as tree shaking, minification, and code splitting. They resolve the dependency graph of your application, ensuring modules are loaded in the correct order. Even with native ESM support in browsers, bundlers remain essential for production because they reduce the number of HTTP requests, eliminate dead code, and enable advanced optimizations that significantly improve load times.',
  codeExample: '// Without a bundler - multiple script tags\n<script src="utils.js"></script>\n<script src="api.js"></script>\n<script src="app.js"></script>\n\n// With a bundler - single optimized file\n<script src="bundle.a1b2c3.js"></script>',
  followUp: 'What are the performance implications of serving unbundled ES modules in production?',
  keyTakeaway: 'Bundlers combine multiple modules into optimized output files, reducing HTTP requests and enabling critical optimizations like tree shaking and minification.',
},
{
  id: 2,
  title: 'What are the main JavaScript module formats (CJS, ESM, AMD, UMD)?',
  difficulty: 'easy',
  topic: 'common-concepts',
  subtopic: 'module-formats',
  answer: 'CommonJS (CJS) is the module system used in Node.js, using require() and module.exports; it loads modules synchronously and is not natively supported in browsers. ES Modules (ESM) is the official JavaScript standard using import/export syntax; it supports static analysis, enabling tree shaking, and is natively supported in modern browsers and Node.js. AMD (Asynchronous Module Definition) was designed for browsers with asynchronous loading via define() and require(), popularized by RequireJS but largely obsolete today. UMD (Universal Module Definition) is a wrapper pattern that works in CJS, AMD, and global script contexts, commonly used for libraries that need to support all environments. Most modern projects use ESM as the primary format, while CJS remains common in the Node.js ecosystem.',
  codeExample: '// CommonJS (CJS)\nconst lodash = require(\'lodash\');\nmodule.exports = { myFunc };\n\n// ES Modules (ESM)\nimport lodash from \'lodash\';\nexport const myFunc = () => {};\n\n// AMD\ndefine([\'lodash\'], function(lodash) {\n  return { myFunc };\n});\n\n// UMD (simplified)\n(function(root, factory) {\n  if (typeof define === \'function\' && define.amd) {\n    define([\'lodash\'], factory);\n  } else if (typeof module === \'object\') {\n    module.exports = factory(require(\'lodash\'));\n  } else {\n    root.myLib = factory(root._);\n  }\n})(this, function(lodash) {\n  return { myFunc };\n});',
  followUp: 'Why can ESM be statically analyzed but CJS cannot, and how does this affect tree shaking?',
  keyTakeaway: 'ESM is the modern standard with static analysis support; CJS is Node.js legacy; AMD and UMD are largely obsolete patterns for cross-environment compatibility.',
},
{
  id: 3,
  title: 'What is tree shaking in the context of bundlers?',
  difficulty: 'easy',
  topic: 'common-concepts',
  subtopic: 'tree-shaking',
  answer: 'Tree shaking is a dead code elimination technique used by bundlers to remove unused exports from the final bundle. The term comes from the idea of shaking a tree and letting dead leaves fall off. It works by leveraging the static structure of ES module imports and exports, where the bundler can determine at build time exactly which exports are used and which are not. For example, if you import only one function from a utility library, tree shaking ensures the other unused functions are excluded from the bundle. This process relies on ESM because import and export declarations are statically analyzable, unlike CommonJS require() which is dynamic. Tree shaking can dramatically reduce bundle sizes, especially when using large utility libraries like lodash-es.',
  codeExample: '// math.js\nexport const add = (a, b) => a + b;\nexport const subtract = (a, b) => a - b;\nexport const multiply = (a, b) => a * b;\n\n// app.js - only imports add\nimport { add } from \'./math.js\';\nconsole.log(add(2, 3));\n\n// After tree shaking, subtract and multiply\n// are removed from the final bundle',
  followUp: 'Why does tree shaking not work with CommonJS modules?',
  keyTakeaway: 'Tree shaking removes unused exports from the bundle by statically analyzing ES module import/export declarations at build time.',
},
{
  id: 4,
  title: 'What is code splitting and why is it important?',
  difficulty: 'easy',
  topic: 'common-concepts',
  subtopic: 'code-splitting',
  answer: 'Code splitting is a technique where the bundler divides the application code into multiple smaller chunks instead of producing a single large bundle. These chunks can then be loaded on demand or in parallel, significantly improving initial page load performance. The most common form is route-based splitting, where each page or route gets its own chunk that is only loaded when the user navigates to that route. Code splitting is important because it reduces the amount of JavaScript that needs to be downloaded, parsed, and executed before the page becomes interactive. Without code splitting, users must download the entire application upfront, including code for pages they may never visit. Modern frameworks like Next.js and React with lazy() and Suspense make code splitting straightforward to implement.',
  codeExample: '// Without code splitting - everything in one bundle\nimport { AdminDashboard } from \'./AdminDashboard\';\nimport { UserProfile } from \'./UserProfile\';\n\n// With code splitting - loaded on demand\nconst AdminDashboard = React.lazy(\n  () => import(\'./AdminDashboard\')\n);\nconst UserProfile = React.lazy(\n  () => import(\'./UserProfile\')\n);\n\n// Usage with Suspense\n<Suspense fallback={<Loading />}>\n  <AdminDashboard />\n</Suspense>',
  followUp: 'What is the difference between route-based and component-based code splitting, and when would you choose each?',
  keyTakeaway: 'Code splitting divides the bundle into smaller chunks loaded on demand, reducing initial load time by only serving the code needed for the current view.',
},
{
  id: 5,
  title: 'What is Hot Module Replacement (HMR)?',
  difficulty: 'easy',
  topic: 'common-concepts',
  subtopic: 'hmr',
  answer: 'Hot Module Replacement (HMR) is a development feature that allows modules to be updated in the browser at runtime without requiring a full page reload. When you save a file, the bundler detects the change, recompiles only the affected module, and sends the update to the browser via a WebSocket connection. The browser then swaps the old module with the new one while preserving the current application state, such as form inputs, scroll position, and component state. This dramatically speeds up the development feedback loop compared to a full reload, which would lose all in-memory state. HMR works best with CSS changes (which are always safely replaceable) and with frameworks like React that support component-level hot reloading through tools like React Fast Refresh. If a hot update cannot be safely applied, HMR will typically fall back to a full page reload.',
  codeExample: '// Webpack HMR API (low-level)\nif (module.hot) {\n  module.hot.accept(\'./myModule\', () => {\n    // Called when myModule or its deps are updated\n    const updated = require(\'./myModule\');\n    render(updated);\n  });\n}\n\n// React Fast Refresh handles this automatically\n// for React components - no manual code needed',
  followUp: 'How does React Fast Refresh differ from the older React Hot Loader approach?',
  keyTakeaway: 'HMR updates changed modules in the browser without a full reload, preserving application state and dramatically speeding up the development feedback loop.',
},
{
  id: 6,
  title: 'How does a bundler construct a dependency graph?',
  difficulty: 'medium',
  topic: 'common-concepts',
  subtopic: 'dependency-graph',
  answer: 'A bundler constructs a dependency graph by starting from one or more entry points and recursively resolving all import and require statements. First, the bundler parses the entry file into an Abstract Syntax Tree (AST) to identify all import declarations and require calls. For each dependency found, the bundler resolves the module specifier to a file path using its resolution algorithm (checking node_modules, file extensions, index files, etc.). It then reads and parses that file, finding its dependencies in turn, continuing recursively until the entire graph is mapped. The resulting dependency graph is a directed acyclic graph (DAG) where each node represents a module and each edge represents an import relationship. This graph is then used to determine the correct bundling order, identify shared dependencies for code splitting, and perform tree shaking by finding unreachable nodes.',
  codeExample: '// Entry: app.js\nimport { render } from \'./renderer\';  // Edge: app -> renderer\nimport { data } from \'./store\';       // Edge: app -> store\n\n// renderer.js\nimport { format } from \'./utils\';     // Edge: renderer -> utils\n\n// store.js\nimport { validate } from \'./utils\';   // Edge: store -> utils\n\n// Dependency Graph:\n//       app.js\n//      /      \\\n// renderer  store\n//      \\      /\n//       utils',
  followUp: 'How does the resolution algorithm decide where to look when you write import \'lodash\' versus import \'./utils\'?',
  keyTakeaway: 'Bundlers build a dependency graph by recursively parsing imports from entry points, resolving file paths, and mapping every module relationship in the application.',
},
{
  id: 7,
  title: 'What is the difference between static and dynamic imports for code splitting?',
  difficulty: 'medium',
  topic: 'common-concepts',
  subtopic: 'code-splitting',
  answer: 'Static imports (import x from \'./module\') are declarations that must appear at the top level of a module and are resolved at compile time; bundlers include statically imported modules in the same chunk as the importing module by default. Dynamic imports (import(\'./module\')) return a Promise and can be used anywhere in code, including inside conditionals, loops, or event handlers; bundlers automatically create a separate chunk for dynamically imported modules. Static imports are hoisted and executed before any module code runs, which enables tree shaking and static analysis. Dynamic imports are evaluated at runtime, loading the chunk only when the import() expression is executed, making them ideal for lazy loading routes, heavy libraries, or conditionally needed features. The trade-off is that static imports provide better optimization opportunities and simpler error handling, while dynamic imports provide better initial load performance through deferred loading.',
  codeExample: '// Static import - included in the main bundle\nimport { heavyCalculation } from \'./math\';\n\n// Dynamic import - creates a separate chunk\nbutton.addEventListener(\'click\', async () => {\n  const { heavyCalculation } = await import(\'./math\');\n  heavyCalculation();\n});\n\n// Dynamic import with webpack magic comments\nconst module = await import(\n  /* webpackChunkName: "math-utils" */\n  /* webpackPrefetch: true */\n  \'./math\'\n);',
  followUp: 'How do bundlers handle dynamic imports where the module path is a variable rather than a string literal?',
  keyTakeaway: 'Static imports are resolved at compile time and included in the same chunk, while dynamic imports create separate chunks loaded on demand at runtime.',
},
{
  id: 8,
  title: 'How do source maps work and what are the different quality levels?',
  difficulty: 'medium',
  topic: 'common-concepts',
  subtopic: 'source-maps',
  answer: 'Source maps are JSON files that create a mapping between the transformed/bundled code and the original source code, enabling developers to debug production code using the original files in browser DevTools. They work by storing a VLQ-encoded mapping of positions: each segment in the generated file maps to a line, column, source file, and original position in that source. The mappings field uses Base64 VLQ encoding for compact representation of these position relationships. Different quality levels exist as trade-offs between build speed and debugging accuracy: "eval" wraps modules in eval() with a sourceURL comment for fast rebuilds but poor quality; "cheap-source-map" maps only to line numbers (not columns) for faster builds; "source-map" provides full mapping with separate .map files for accurate debugging but slower builds. In production, you can generate source maps but not serve them publicly by uploading them to error monitoring services like Sentry, keeping debug capability without exposing source code to users.',
  codeExample: '// A source map file structure\n{\n  "version": 3,\n  "file": "bundle.js",\n  "sources": ["src/app.ts", "src/utils.ts"],\n  "sourcesContent": ["original source..."],\n  "names": ["myFunc", "result"],\n  "mappings": "AAAA,SAAS;AACT,MAAM..."\n}\n\n// Referenced in the bundle via comment:\n//# sourceMappingURL=bundle.js.map\n\n// Or via HTTP header:\nSourceMap: /path/to/bundle.js.map',
  followUp: 'What are the security considerations of deploying source maps to production?',
  keyTakeaway: 'Source maps encode position mappings between bundled and original code using VLQ encoding, with different quality levels trading build speed for debugging accuracy.',
},
{
  id: 9,
  title: 'What is scope hoisting and how does it reduce bundle size?',
  difficulty: 'medium',
  topic: 'common-concepts',
  subtopic: 'scope-hoisting',
  answer: 'Scope hoisting (also called module concatenation) is a bundler optimization that merges multiple modules into a single scope instead of wrapping each module in an individual function closure. Without scope hoisting, bundlers wrap every module in a function like __webpack_require__ to isolate its scope, adding boilerplate overhead for each module. With scope hoisting, the bundler analyzes the dependency graph and safely concatenates modules that form a dependency chain into a single function scope, eliminating the wrapper functions. This reduces bundle size by removing the per-module function wrappers and variable declarations, and it improves runtime performance because JavaScript engines can optimize a single large scope more efficiently than many small closures. Scope hoisting works best with ES modules because their static structure allows the bundler to safely determine that merging scopes will not change behavior. Rollup pioneered this technique and enables it by default, while Webpack added it in version 3 via the ModuleConcatenationPlugin.',
  codeExample: '// WITHOUT scope hoisting (traditional bundling)\n// Each module wrapped in a function\nvar modules = {\n  "./utils.js": function(module, exports) {\n    exports.add = (a, b) => a + b;\n  },\n  "./app.js": function(module, exports, require) {\n    var utils = require("./utils.js");\n    console.log(utils.add(1, 2));\n  }\n};\n\n// WITH scope hoisting (concatenated)\n// Modules merged into single scope\nconst add = (a, b) => a + b;\nconsole.log(add(1, 2));',
  followUp: 'What conditions prevent a module from being scope-hoisted?',
  keyTakeaway: 'Scope hoisting concatenates modules into a single scope, eliminating per-module function wrappers to reduce bundle size and improve runtime performance.',
},
{
  id: 10,
  title: 'What is the difference between bundling, transpiling, and minifying?',
  difficulty: 'medium',
  topic: 'common-concepts',
  subtopic: 'bundling-basics',
  answer: 'Bundling is the process of combining multiple modules and their dependencies into one or more output files, resolving the dependency graph and ensuring correct module loading order. Transpiling (or compiling) converts code from one syntax to another while preserving functionality, such as converting TypeScript to JavaScript, JSX to React.createElement calls, or modern ES2024+ syntax to ES5 for older browsers using tools like Babel, TypeScript compiler, or SWC. Minifying reduces the file size of already-bundled code by removing whitespace, shortening variable names, eliminating dead code paths, and applying other size optimizations without changing behavior, using tools like Terser, esbuild, or SWC. These three processes typically happen in sequence during a production build: transpiling first converts the source to compatible JavaScript, bundling then combines all modules into output chunks, and minifying finally compresses those chunks for the smallest possible file size. Modern tools like esbuild and SWC combine all three steps into a single pass for dramatically faster builds.',
  codeExample: '// 1. ORIGINAL (TypeScript + JSX)\nconst App: React.FC = () => <h1>{getMessage()}</h1>;\n\n// 2. TRANSPILED (plain JavaScript)\nconst App = () => React.createElement("h1", null, getMessage());\n\n// 3. BUNDLED (dependencies inlined)\nfunction getMessage() { return "Hello"; }\nconst App = () => React.createElement("h1", null, getMessage());\n\n// 4. MINIFIED (compressed)\nfunction n(){return"Hello"}const e=()=>React.createElement("h1",null,n());',
  followUp: 'Why are tools like esbuild and SWC orders of magnitude faster than Babel and Terser?',
  keyTakeaway: 'Bundling combines modules, transpiling converts syntax for compatibility, and minifying compresses the output; these are three distinct build steps often run in sequence.',
},
{
  id: 11,
  title: 'How does tree shaking handle side effects, and what is the sideEffects field in package.json?',
  difficulty: 'hard',
  topic: 'common-concepts',
  subtopic: 'side-effects',
  answer: 'Tree shaking must be conservative with modules that have side effects because removing an unused export from a side-effectful module could break functionality that depends on the side effect executing at import time. A side effect is any code that runs during module evaluation and affects something outside its own scope, such as modifying globals, polyfilling prototypes, registering event listeners, or writing to the DOM. The sideEffects field in package.json tells the bundler which files are safe to tree shake entirely when none of their exports are used. Setting "sideEffects": false declares that every file in the package is side-effect-free, allowing aggressive tree shaking. You can also provide an array like "sideEffects": ["./src/polyfills.js", "*.css"] to specify which files DO have side effects and must always be included. Without this field, bundlers must assume every module could have side effects, severely limiting tree shaking effectiveness, which is why properly configuring sideEffects is one of the most impactful optimizations for library authors.',
  codeExample: '// package.json\n{\n  "name": "my-library",\n  "sideEffects": false\n  // All files are side-effect-free, safe to tree shake\n}\n\n// OR specify files with side effects\n{\n  "sideEffects": [\n    "./src/polyfills.js",\n    "*.css",\n    "*.global.js"\n  ]\n}\n\n// Side-effectful module (NOT safe to remove)\n// polyfills.js\nArray.prototype.flat = Array.prototype.flat || function() { /*...*/ };\n\n// Side-effect-free module (safe to tree shake)\n// utils.js\nexport const add = (a, b) => a + b;\nexport const multiply = (a, b) => a * b;',
  followUp: 'How do you use the /*#__PURE__*/ annotation to mark specific function calls as side-effect-free?',
  keyTakeaway: 'The sideEffects field in package.json tells bundlers which files are safe to completely remove when their exports are unused, enabling dramatically more effective tree shaking.',
},
{
  id: 12,
  title: 'What are the trade-offs between different chunking strategies (vendor, route-based, shared)?',
  difficulty: 'hard',
  topic: 'common-concepts',
  subtopic: 'chunking',
  answer: 'Vendor chunking separates third-party dependencies into a dedicated bundle that changes infrequently, maximizing cache hits since application code changes far more often than dependencies; however, a single large vendor chunk means users download all library code upfront even if some libraries are only used on specific pages. Route-based chunking creates a separate chunk per route or page, ensuring users only download code for the page they are visiting; the downside is that shared dependencies may be duplicated across route chunks or require additional shared chunks, increasing complexity. Shared chunking identifies modules imported by multiple entry points and extracts them into common chunks, reducing duplication; but overly aggressive shared chunking can create many small files that increase HTTP overhead, or create a large common chunk that delays initial load. The optimal strategy is usually a combination: separate long-lived vendor chunks for stable dependencies, route-based chunks for page-specific code, and intelligently sized shared chunks for code used across multiple routes. The key metrics to balance are initial load size, cache efficiency, chunk count (HTTP requests), and duplication across chunks.',
  codeExample: '// Webpack SplitChunksPlugin - combined strategy\noptimization: {\n  splitChunks: {\n    chunks: \'all\',\n    cacheGroups: {\n      // Vendor chunk for stable dependencies\n      vendor: {\n        test: /[\\\\/]node_modules[\\\\/]/,\n        name: \'vendors\',\n        priority: 10,\n      },\n      // Shared chunk for code used by 2+ routes\n      common: {\n        minChunks: 2,\n        name: \'common\',\n        priority: 5,\n        reuseExistingChunk: true,\n      },\n    },\n  },\n}\n// Route chunks created automatically via dynamic imports',
  followUp: 'How would you determine the optimal minSize and maxSize thresholds for your chunking strategy?',
  keyTakeaway: 'The best chunking strategy combines vendor splitting for cache longevity, route-based splitting for minimal initial load, and shared chunks to reduce duplication without excessive HTTP requests.',
},
{
  id: 13,
  title: 'How would you debug a production bundle to identify why tree shaking failed for a module?',
  difficulty: 'hard',
  topic: 'common-concepts',
  subtopic: 'tree-shaking',
  answer: 'Start by analyzing the bundle with a visualization tool like webpack-bundle-analyzer, source-map-explorer, or rollup-plugin-visualizer to identify which modules are unexpectedly large or present. Check if the module uses CommonJS format instead of ESM, as CJS cannot be tree shaken due to dynamic require() calls; inspect the module\'s package.json to see if it has a "module" or "exports" field pointing to an ESM build. Verify that the package.json has a "sideEffects" field, because without it the bundler assumes all files have side effects and cannot remove unused imports. Look for barrel files (index.js re-exporting everything) which can defeat tree shaking if any re-exported module has side effects. Check if the code contains patterns the bundler cannot statically analyze, such as computed property access (obj[key]), object spread of the entire module, or dynamic property assignments. You can use Webpack\'s stats output with --stats-reasons to see exactly why each module was included, or use the concatenatedModules optimization stats to see which modules were not hoisted.',
  codeExample: '// Common tree shaking failures:\n\n// 1. CJS format (cannot tree shake)\nconst { pick } = require(\'lodash\');  // Pulls entire lodash\nimport { pick } from \'lodash-es\';    // Tree shakeable\n\n// 2. Barrel file with side effects\n// index.js re-exports everything\nexport { Chart } from \'./Chart\';     // Chart has side effects\nexport { Table } from \'./Table\';     // Table is what we need\n// Bundler must include Chart due to side effects\n\n// 3. Debugging with Webpack stats\n// npx webpack --stats-reasons --json > stats.json\n// Look for "reasons" array in each module\n\n// 4. Check with WEBPACK_BUNDLE_ANALYZER\n// npm run build -- --env analyze\nplugins: [\n  new BundleAnalyzerPlugin()\n]',
  followUp: 'How do barrel files (index.ts re-exports) interact with tree shaking, and when should you avoid them?',
  keyTakeaway: 'Debug tree shaking failures by checking module format (must be ESM), sideEffects field, barrel file re-exports, and using bundle analysis tools to trace why specific modules were included.',
},
{
  id: 14,
  title: 'Explain how circular dependencies are handled by bundlers and what problems they cause.',
  difficulty: 'hard',
  topic: 'common-concepts',
  subtopic: 'circular-deps',
  answer: 'Circular dependencies occur when module A imports module B and module B imports module A (directly or through a chain). Bundlers handle this by partially evaluating modules: when a circular import is encountered, the importing module receives the partially initialized exports object of the dependency, which may be incomplete or undefined at the time of access. In CommonJS, require() returns whatever module.exports is at the point the circular dependency is encountered, which may be an empty object if the exporting module has not yet assigned its exports. In ESM, imports are live bindings (references), so the value will eventually be correct once the module finishes evaluating, but accessing the binding before the module has completed evaluation throws a TDZ (Temporal Dead Zone) ReferenceError. Circular dependencies cause subtle bugs like undefined function calls, incomplete objects, and initialization order issues that are difficult to debug because they may only manifest in certain build configurations or import orders. The best practice is to refactor circular dependencies by extracting shared code into a third module that both original modules can import.',
  codeExample: '// Circular dependency example\n// moduleA.js\nimport { funcB } from \'./moduleB\';\nexport const funcA = () => \'A\';\nconsole.log(funcB()); // May be undefined!\n\n// moduleB.js\nimport { funcA } from \'./moduleA\';\nexport const funcB = () => \'B\';\nconsole.log(funcA()); // May be undefined!\n\n// FIX: Extract shared dependency\n// shared.js\nexport const funcA = () => \'A\';\nexport const funcB = () => \'B\';\n\n// moduleA.js\nimport { funcB } from \'./shared\';\n// moduleB.js\nimport { funcA } from \'./shared\';\n\n// Detect circular deps with:\n// npx madge --circular src/',
  followUp: 'What tools can you use to detect and visualize circular dependencies in a large codebase?',
  keyTakeaway: 'Circular dependencies cause partially initialized exports and undefined values because modules receive incomplete bindings during evaluation; refactor by extracting shared code into a third module.',
},
{
  id: 15,
  title: 'How does differential serving work with bundlers (modern vs legacy builds)?',
  difficulty: 'hard',
  topic: 'common-concepts',
  subtopic: 'bundling-basics',
  answer: 'Differential serving produces two separate bundles from the same source code: a modern bundle targeting current browsers with native ES2020+ support, and a legacy bundle transpiled down to ES5 with polyfills for older browsers. The modern bundle is significantly smaller because it skips transpilation of features like arrow functions, async/await, optional chaining, and destructuring, and it omits polyfills that modern browsers already implement natively. The HTML serves both bundles using the module/nomodule pattern: modern browsers load the script with type="module" and ignore the nomodule script, while legacy browsers ignore type="module" scripts and load the nomodule fallback. This approach can reduce the modern bundle size by 20-40% compared to a single bundle targeting the lowest common denominator. The trade-off is increased build time (two full builds), more complex deployment configuration, and potential edge cases with browsers that support modules but lack certain modern features. Tools like @babel/preset-env with browserslist targets, Webpack\'s target configuration, and Vite\'s @vitejs/plugin-legacy automate this process.',
  codeExample: '<!-- Differential serving with module/nomodule -->\n<script type="module" src="app.modern.js"></script>\n<script nomodule src="app.legacy.js"></script>\n\n// Webpack configuration for two builds\n// webpack.modern.js\nmodule.exports = {\n  target: \'web\',\n  output: { filename: \'[name].modern.js\' },\n  module: {\n    rules: [{\n      test: /\\.js$/,\n      use: {\n        loader: \'babel-loader\',\n        options: {\n          presets: [[\'@babel/preset-env\', {\n            targets: { esmodules: true }\n          }]]\n        }\n      }\n    }]\n  }\n};\n\n// Vite with legacy plugin\nimport legacy from \'@vitejs/plugin-legacy\';\nexport default {\n  plugins: [\n    legacy({ targets: [\'defaults\', \'not IE 11\'] })\n  ]\n};',
  followUp: 'What are the known edge cases with the module/nomodule pattern, such as Safari 10.1 double-downloading?',
  keyTakeaway: 'Differential serving produces modern and legacy bundles, using the module/nomodule pattern to deliver smaller, un-transpiled code to modern browsers while maintaining backward compatibility.',
},
{
  id: 16,
  title: 'What are the core concepts of Webpack (entry, output, loaders, plugins)?',
  difficulty: 'easy',
  topic: 'webpack',
  subtopic: 'core-concepts',
  answer: 'Webpack has four core concepts that form the foundation of every configuration. Entry is the starting point(s) of the dependency graph; Webpack begins resolving imports from the entry file, and you can specify one or multiple entries for multi-page applications. Output defines where the bundled files are written and how they are named, typically using the path and filename properties with optional hash-based naming for cache busting. Loaders transform files that are not JavaScript into modules that Webpack can process; they work at the individual file level during the module loading phase, specified with test (regex to match files) and use (which loader to apply). Plugins extend Webpack\'s capabilities by hooking into the build lifecycle at various stages; they can modify the entire compilation, generate additional files, or optimize the output. Together, these four concepts allow Webpack to take any type of file as input and produce optimized bundles as output.',
  codeExample: '// webpack.config.js\nconst HtmlWebpackPlugin = require(\'html-webpack-plugin\');\nconst path = require(\'path\');\n\nmodule.exports = {\n  // Entry: where to start bundling\n  entry: \'./src/index.js\',\n\n  // Output: where to write the bundle\n  output: {\n    path: path.resolve(__dirname, \'dist\'),\n    filename: \'[name].[contenthash].js\',\n  },\n\n  // Loaders: transform non-JS files\n  module: {\n    rules: [\n      { test: /\\.css$/, use: [\'style-loader\', \'css-loader\'] },\n      { test: /\\.tsx?$/, use: \'ts-loader\' },\n    ],\n  },\n\n  // Plugins: extend build capabilities\n  plugins: [\n    new HtmlWebpackPlugin({ template: \'./src/index.html\' }),\n  ],\n};',
  followUp: 'What is the difference between the mode property values (development, production, none) and what optimizations does each enable?',
  keyTakeaway: 'Webpack\'s four pillars are entry (where to start), output (where to write), loaders (file transformations), and plugins (build lifecycle hooks).',
},
{
  id: 17,
  title: 'What is a Webpack loader and how does it differ from a plugin?',
  difficulty: 'easy',
  topic: 'webpack',
  subtopic: 'loaders',
  answer: 'A Webpack loader is a function that transforms individual files as they are imported into the dependency graph, taking the source content as input and returning the transformed content. Loaders operate at the module level, processing one file at a time during the module resolution phase, and they are chained right-to-left (or bottom-to-top in config), where each loader passes its output to the next. For example, for SCSS processing: sass-loader compiles SCSS to CSS, css-loader resolves CSS imports, and style-loader injects the CSS into the DOM. A plugin, in contrast, operates on the entire compilation and can hook into any phase of the Webpack build lifecycle using the tapable hooks system. Plugins have access to the compiler and compilation objects, allowing them to modify chunks, optimize assets, generate additional files, or alter the build behavior globally. The key distinction is scope: loaders transform individual files one at a time, while plugins can affect the entire build process across all files.',
  codeExample: '// Loader: transforms individual files\nmodule: {\n  rules: [\n    {\n      test: /\\.scss$/,\n      // Loaders execute right to left:\n      // sass-loader -> css-loader -> style-loader\n      use: [\'style-loader\', \'css-loader\', \'sass-loader\'],\n    },\n  ],\n},\n\n// Plugin: hooks into the build lifecycle\nconst { BundleAnalyzerPlugin } = require(\'webpack-bundle-analyzer\');\n\nplugins: [\n  // Operates on the entire build output\n  new BundleAnalyzerPlugin(),\n  new MiniCssExtractPlugin({ filename: \'[name].[contenthash].css\' }),\n]',
  followUp: 'In what order are loaders executed when multiple loaders are configured for the same file type?',
  keyTakeaway: 'Loaders transform individual files during module resolution (one at a time, chained right-to-left), while plugins hook into the entire build lifecycle to modify the compilation globally.',
},
{
  id: 18,
  title: 'What is the purpose of webpack.config.js?',
  difficulty: 'easy',
  topic: 'webpack',
  subtopic: 'core-concepts',
  answer: 'webpack.config.js is the configuration file that tells Webpack how to build your project, defining the entry points, output location, loaders for different file types, plugins for build optimizations, and other settings. By default, Webpack looks for this file in the project root directory, but you can specify a different config file using the --config flag. The config file exports a JavaScript object (or a function that returns one), giving you the full power of JavaScript to dynamically construct configurations based on environment variables, arguments, or other conditions. When exported as a function, it receives the env and argv parameters, allowing you to create different configurations for development and production from a single file. You can also export an array of configurations for multi-target builds (e.g., building both a client and server bundle). While Webpack 5 has sensible defaults and can work with zero configuration for simple projects, real-world applications almost always need a custom config to handle TypeScript, CSS preprocessors, code splitting, and optimization settings.',
  codeExample: '// webpack.config.js - basic setup\nmodule.exports = {\n  entry: \'./src/index.js\',\n  output: { filename: \'bundle.js\' },\n};\n\n// Function form for environment-based config\nmodule.exports = (env, argv) => ({\n  mode: env.production ? \'production\' : \'development\',\n  devtool: env.production ? \'source-map\' : \'eval-cheap-source-map\',\n  entry: \'./src/index.js\',\n  output: {\n    filename: env.production\n      ? \'[name].[contenthash].js\'\n      : \'[name].js\',\n  },\n});\n\n// CLI: npx webpack --config webpack.prod.js',
  followUp: 'What is the difference between exporting an object, a function, and an array from webpack.config.js?',
  keyTakeaway: 'webpack.config.js defines the entire build pipeline including entry points, output, loaders, plugins, and optimizations, and supports dynamic configuration via function exports.',
},
{
  id: 19,
  title: 'How do you configure Webpack to handle CSS files?',
  difficulty: 'easy',
  topic: 'webpack',
  subtopic: 'loaders',
  answer: 'Webpack does not understand CSS natively, so you need loaders to process CSS files when they are imported in JavaScript. The minimum setup requires two loaders: css-loader resolves @import and url() statements within CSS files and converts them into JavaScript module dependencies, while style-loader injects the resulting CSS into the DOM by creating <style> tags at runtime. For production builds, you should replace style-loader with MiniCssExtractPlugin, which extracts CSS into separate .css files that can be cached independently and loaded in parallel with JavaScript. For CSS preprocessors like Sass or Less, add the corresponding loader (sass-loader, less-loader) to the chain before css-loader. For CSS Modules, enable the modules option in css-loader to scope class names locally and prevent naming collisions. PostCSS can be added via postcss-loader to apply autoprefixing, future CSS syntax, and other transformations.',
  codeExample: '// Development: inject CSS into DOM\nmodule: {\n  rules: [{\n    test: /\\.css$/,\n    use: [\'style-loader\', \'css-loader\'],\n  }],\n},\n\n// Production: extract to separate files\nconst MiniCssExtractPlugin = require(\'mini-css-extract-plugin\');\nmodule: {\n  rules: [{\n    test: /\\.css$/,\n    use: [MiniCssExtractPlugin.loader, \'css-loader\'],\n  }],\n},\nplugins: [new MiniCssExtractPlugin()],\n\n// With Sass + CSS Modules + PostCSS\n{\n  test: /\\.module\\.scss$/,\n  use: [\n    MiniCssExtractPlugin.loader,\n    { loader: \'css-loader\', options: { modules: true } },\n    \'postcss-loader\',\n    \'sass-loader\',\n  ],\n}',
  followUp: 'What is the difference between style-loader and MiniCssExtractPlugin, and when should you use each?',
  keyTakeaway: 'CSS handling requires css-loader to resolve imports and either style-loader (dev, injects <style> tags) or MiniCssExtractPlugin (prod, extracts .css files) to deliver the styles.',
},
{
  id: 20,
  title: 'What is Webpack Dev Server?',
  difficulty: 'easy',
  topic: 'webpack',
  subtopic: 'dev-server',
  answer: 'Webpack Dev Server (webpack-dev-server) is a development HTTP server that serves your Webpack bundles from memory with live reloading and Hot Module Replacement support. Unlike a production build that writes files to disk, the dev server compiles bundles in memory for faster rebuild times and serves them via an Express-based HTTP server. It watches your source files for changes, automatically recompiles the affected modules, and pushes updates to the browser through a WebSocket connection. Key features include HMR for applying changes without a full reload, a proxy configuration for routing API requests to a backend server during development, historyApiFallback for single-page applications using client-side routing, and HTTPS support for testing secure features. The dev server is configured through the devServer property in webpack.config.js and is started with the webpack serve command.',
  codeExample: '// webpack.config.js\nmodule.exports = {\n  devServer: {\n    port: 3000,\n    hot: true,                    // Enable HMR\n    open: true,                   // Open browser on start\n    historyApiFallback: true,     // SPA routing support\n    proxy: [\n      {\n        context: [\'/api\'],\n        target: \'http://localhost:8080\', // Proxy API requests\n        changeOrigin: true,\n      },\n    ],\n    static: {\n      directory: \'./public\',      // Serve static files\n    },\n    client: {\n      overlay: true,              // Show errors as overlay\n    },\n  },\n};\n\n// Start: npx webpack serve',
  followUp: 'How does Webpack Dev Server\'s proxy configuration help avoid CORS issues during development?',
  keyTakeaway: 'Webpack Dev Server serves bundles from memory with HMR, live reloading, API proxying, and instant recompilation, providing a fast development feedback loop.',
},
{
  id: 21,
  title: 'How does Webpack\'s chunk splitting work with SplitChunksPlugin?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'chunk-splitting',
  answer: 'SplitChunksPlugin is Webpack\'s built-in optimization that automatically identifies modules shared between chunks and extracts them into separate shared chunks to reduce duplication. It works by analyzing which modules are imported by multiple entry points or dynamic import chunks, then applying a set of configurable conditions to decide whether splitting is beneficial. The key conditions are: chunks (which chunks to analyze: all, async, or initial), minSize (minimum size before a module is split out, default 20KB), minChunks (minimum number of chunks that must share the module, default 1), maxAsyncRequests (maximum parallel requests for on-demand chunks), and maxInitialRequests (maximum parallel requests for entry point chunks). Cache groups allow you to define custom splitting rules with priorities: for example, separating node_modules into a vendors chunk and application shared code into a commons chunk. The algorithm evaluates all candidate modules against these conditions and splits only when doing so would result in a net benefit after considering the overhead of additional HTTP requests.',
  codeExample: 'optimization: {\n  splitChunks: {\n    chunks: \'all\',          // Analyze all chunk types\n    minSize: 20000,          // Min 20KB to split\n    minChunks: 1,            // Min shared by 1 chunk\n    maxAsyncRequests: 30,    // Max parallel async loads\n    maxInitialRequests: 30,  // Max parallel initial loads\n    cacheGroups: {\n      // Split node_modules into vendor chunk\n      defaultVendors: {\n        test: /[\\\\/]node_modules[\\\\/]/,\n        priority: -10,\n        reuseExistingChunk: true,\n        name: \'vendors\',\n      },\n      // Split shared app code\n      common: {\n        minChunks: 2,\n        priority: -20,\n        reuseExistingChunk: true,\n      },\n      // Split large libraries individually\n      react: {\n        test: /[\\\\/]node_modules[\\\\/](react|react-dom)[\\\\/]/,\n        name: \'react-vendor\',\n        priority: 10,\n      },\n    },\n  },\n}',
  followUp: 'What is the difference between chunks: "all", "async", and "initial", and when would you use each?',
  keyTakeaway: 'SplitChunksPlugin extracts shared modules into separate chunks based on configurable size thresholds, share counts, and cache group rules to reduce duplication and improve caching.',
},
{
  id: 22,
  title: 'What is the difference between hash, chunkhash, and contenthash in Webpack?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'hashing',
  answer: 'Webpack offers three hashing strategies for output filenames, each with different scopes for cache invalidation. Hash (now called fullhash in Webpack 5) generates a single hash for the entire build; if any file in the project changes, every output filename changes, making it the least cache-friendly option. Chunkhash generates a hash based on the contents of each individual chunk and all its dependencies; if a module in one chunk changes, only that chunk\'s hash changes while other chunks remain cacheable. However, chunkhash links JavaScript and extracted CSS from the same chunk, so a JS-only change would also invalidate the CSS file\'s cache. Contenthash generates a hash based purely on the extracted content of each individual file; this means CSS extracted by MiniCssExtractPlugin gets its own hash independent of the JavaScript chunk it came from. For production builds, contenthash is the recommended strategy because it provides the most granular cache invalidation: only files whose actual content changed get new hashes, maximizing long-term browser caching.',
  codeExample: '// Using different hash types\nmodule.exports = {\n  output: {\n    // fullhash: changes if ANY file changes\n    filename: \'[name].[fullhash].js\',\n\n    // chunkhash: changes if chunk content changes\n    filename: \'[name].[chunkhash].js\',\n\n    // contenthash: changes only for that specific file\n    filename: \'[name].[contenthash].js\',  // Recommended\n  },\n  plugins: [\n    new MiniCssExtractPlugin({\n      // CSS gets its own contenthash, independent of JS\n      filename: \'[name].[contenthash].css\',\n    }),\n  ],\n};\n\n// Output examples:\n// main.a1b2c3d4.js   (JS changed = new hash)\n// main.e5f6g7h8.css  (CSS unchanged = same hash)',
  followUp: 'How does Webpack 5\'s deterministic moduleIds and chunkIds option prevent unnecessary cache invalidation?',
  keyTakeaway: 'Use contenthash for production builds because it hashes each file\'s content independently, ensuring only files with actual changes get new filenames and other files remain cached.',
},
{
  id: 23,
  title: 'How do you configure Webpack for different environments (dev vs prod)?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'environments',
  answer: 'The most common approach is to create separate configuration files for shared, development, and production settings, then merge them using webpack-merge. The base config (webpack.common.js) contains shared settings like entry points, loader rules, and resolve configuration. The development config (webpack.dev.js) merges with the base and adds development-specific settings: mode "development" for readable output, eval-cheap-module-source-map for fast source maps, the dev server configuration, and HMR plugins. The production config (webpack.prod.js) merges with the base and adds: mode "production" for built-in optimizations (tree shaking, minification), source-map devtool for full source maps, MiniCssExtractPlugin for CSS extraction, CssMinimizerPlugin for CSS minification, and contenthash in filenames for long-term caching. Webpack\'s mode property automatically sets process.env.NODE_ENV, enables/disables specific plugins, and configures optimization defaults. You switch between configs using the --config flag in your npm scripts.',
  codeExample: '// webpack.common.js\nmodule.exports = {\n  entry: \'./src/index.js\',\n  module: { rules: [{ test: /\\.tsx?$/, use: \'ts-loader\' }] },\n  resolve: { extensions: [\'.tsx\', \'.ts\', \'.js\'] },\n};\n\n// webpack.dev.js\nconst { merge } = require(\'webpack-merge\');\nconst common = require(\'./webpack.common.js\');\nmodule.exports = merge(common, {\n  mode: \'development\',\n  devtool: \'eval-cheap-module-source-map\',\n  devServer: { hot: true, port: 3000 },\n});\n\n// webpack.prod.js\nconst { merge } = require(\'webpack-merge\');\nconst common = require(\'./webpack.common.js\');\nconst MiniCssExtractPlugin = require(\'mini-css-extract-plugin\');\nmodule.exports = merge(common, {\n  mode: \'production\',\n  devtool: \'source-map\',\n  output: { filename: \'[name].[contenthash].js\' },\n  plugins: [new MiniCssExtractPlugin()],\n});\n\n// package.json scripts\n// "dev": "webpack serve --config webpack.dev.js"\n// "build": "webpack --config webpack.prod.js"',
  followUp: 'What specific optimizations does Webpack automatically enable when mode is set to "production"?',
  keyTakeaway: 'Use webpack-merge to combine a shared base config with environment-specific configs, leveraging mode for automatic optimization defaults and separate files for dev/prod settings.',
},
{
  id: 24,
  title: 'What is Webpack\'s Module Federation and when would you use it?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'module-federation',
  answer: 'Module Federation is a Webpack 5 feature that allows multiple independently built and deployed applications to share code at runtime, enabling micro-frontend architectures without the overhead of npm packages or monorepo builds. Each application can expose specific modules as a "remote" and consume modules from other applications as a "host," with the federation runtime handling loading and version resolution dynamically. Unlike traditional code sharing via npm packages (which requires rebuilding consumers when the package updates), Module Federation loads shared modules at runtime from a remote URL, meaning a team can deploy updates to their exposed modules without requiring any other team to rebuild. You would use Module Federation when building micro-frontends where independent teams own different parts of the UI, when sharing components or utilities across multiple independently deployed applications, or when you want to compose a dashboard from multiple standalone apps. The shared configuration ensures that common dependencies like React are loaded only once, even when multiple remotes declare them as dependencies.',
  codeExample: '// Remote app (exposes components)\n// webpack.config.js\nconst { ModuleFederationPlugin } = require(\'webpack\').container;\nmodule.exports = {\n  plugins: [\n    new ModuleFederationPlugin({\n      name: \'remoteApp\',\n      filename: \'remoteEntry.js\',\n      exposes: {\n        \'./Button\': \'./src/components/Button\',\n        \'./utils\': \'./src/utils\',\n      },\n      shared: [\'react\', \'react-dom\'],\n    }),\n  ],\n};\n\n// Host app (consumes remote modules)\nmodule.exports = {\n  plugins: [\n    new ModuleFederationPlugin({\n      name: \'hostApp\',\n      remotes: {\n        remoteApp: \'remoteApp@http://localhost:3001/remoteEntry.js\',\n      },\n      shared: [\'react\', \'react-dom\'],\n    }),\n  ],\n};\n\n// Usage in host app\nconst RemoteButton = React.lazy(\n  () => import(\'remoteApp/Button\')\n);',
  followUp: 'How do you handle version mismatches when two federated apps depend on different versions of the same library?',
  keyTakeaway: 'Module Federation enables independently built applications to share code at runtime, powering micro-frontend architectures where teams can deploy updates without requiring consumers to rebuild.',
},
{
  id: 25,
  title: 'How does Webpack\'s resolver work and what is the resolve.alias configuration?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'resolver',
  answer: 'Webpack\'s resolver is the module that translates import specifiers (like \'./utils\', \'lodash\', or \'@/components/Button\') into absolute file paths on disk. For relative imports, it looks relative to the importing file\'s directory. For bare specifiers (no leading ./ or ../), it searches through node_modules directories going up the file tree, following the Node.js resolution algorithm. The resolver checks the resolve.extensions array to try different file extensions (e.g., .ts, .tsx, .js), looks at the resolve.mainFields in package.json to determine which entry point to use (e.g., "module" for ESM, "main" for CJS), and follows resolve.mainFiles (default: "index") when importing a directory. The resolve.alias configuration creates mapping shortcuts: you can map \'@\' to your src directory to avoid long relative paths, or redirect imports entirely (e.g., aliasing \'lodash\' to \'lodash-es\' for tree shaking). Aliases are applied before any other resolution steps, making them powerful for controlling which exact file or package is loaded.',
  codeExample: 'resolve: {\n  // File extensions to try (in order)\n  extensions: [\'.tsx\', \'.ts\', \'.jsx\', \'.js\'],\n\n  // Path aliases\n  alias: {\n    \'@\': path.resolve(__dirname, \'src\'),\n    \'@components\': path.resolve(__dirname, \'src/components\'),\n    // Redirect lodash to ESM version for tree shaking\n    \'lodash\': \'lodash-es\',\n    // Replace a module entirely\n    \'old-lib$\': path.resolve(__dirname, \'src/shims/old-lib.js\'),\n  },\n\n  // Which package.json fields to check\n  mainFields: [\'module\', \'browser\', \'main\'],\n\n  // Directories to search for modules\n  modules: [\'node_modules\', path.resolve(__dirname, \'src\')],\n}\n\n// Now these imports work:\nimport { Button } from \'@/components/Button\';\nimport { pick } from \'lodash\'; // actually loads lodash-es',
  followUp: 'What is the difference between resolve.alias and tsconfig paths, and do you need both?',
  keyTakeaway: 'Webpack\'s resolver translates import specifiers to file paths using configurable extensions, main fields, and aliases that can create path shortcuts or redirect entire packages.',
},
{
  id: 26,
  title: 'What are Webpack\'s different devtool options for source maps?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'source-maps',
  answer: 'Webpack provides over a dozen devtool options that combine keywords (eval, cheap, module, source-map, nosources, hidden) to create different source map strategies with varying trade-offs between build speed, rebuild speed, and debugging quality. "eval" wraps each module in eval() with a sourceURL comment, providing the fastest rebuilds but only mapping to module filenames without line-level accuracy. "source-map" generates a full separate .map file with accurate line and column mapping but is the slowest to build. "cheap-source-map" maps only to line numbers (no column mapping), which is faster to generate. Adding "module" (e.g., "cheap-module-source-map") ensures source maps trace back to the original pre-loader source (like TypeScript or JSX) rather than the loader output. "hidden-source-map" generates the .map file but does not add the reference comment in the bundle, useful for uploading maps to error tracking services without exposing them to users. For development, "eval-cheap-module-source-map" offers the best balance of rebuild speed and debugging quality; for production, "source-map" or "hidden-source-map" is recommended.',
  codeExample: '// Development - fast rebuilds, good quality\ndevtool: \'eval-cheap-module-source-map\',\n\n// Production - full quality, separate .map file\ndevtool: \'source-map\',\n\n// Production - maps generated but not referenced\n// (upload to Sentry/Datadog, not exposed to users)\ndevtool: \'hidden-source-map\',\n\n// Production - no source content in map\n// (shows file/line but not original code)\ndevtool: \'nosources-source-map\',\n\n// Speed comparison (fastest to slowest):\n// eval                         ~fastest rebuild\n// eval-cheap-source-map\n// eval-cheap-module-source-map ~best dev balance\n// cheap-module-source-map\n// source-map                   ~slowest, best quality',
  followUp: 'Why does the "eval" keyword dramatically speed up rebuilds compared to generating a proper source map file?',
  keyTakeaway: 'Use eval-cheap-module-source-map for fast development rebuilds with decent debugging, and source-map or hidden-source-map in production for full accuracy.',
},
{
  id: 27,
  title: 'How do you analyze and optimize a Webpack bundle size?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'bundle-analysis',
  answer: 'Start by generating a visual analysis using webpack-bundle-analyzer, which creates an interactive treemap showing every module\'s size contribution across all chunks. Install it as a plugin or use the --analyze flag with webpack-cli to generate a report. Look for the biggest modules first: oversized node_modules dependencies, unexpectedly included packages, and duplicated modules across chunks. Common optimizations include: replacing heavy libraries with lighter alternatives (e.g., date-fns instead of moment.js), using dynamic imports to code-split rarely-used features, configuring the externals option to exclude libraries provided by a CDN, and ensuring tree shaking is working by checking that imports use ESM-compatible packages. Use the stats output (webpack --json > stats.json) to examine detailed module sizes and inclusion reasons, or upload it to tools like webpack.github.io/analyse for a graph view. Additionally, ensure images and fonts use asset modules with size limits, CSS is extracted and minified, and compression (gzip/brotli) is configured for your deployment.',
  codeExample: '// Add bundle analyzer plugin\nconst BundleAnalyzerPlugin = require(\n  \'webpack-bundle-analyzer\'\n).BundleAnalyzerPlugin;\n\nplugins: [\n  new BundleAnalyzerPlugin({\n    analyzerMode: \'static\',  // Generate HTML report\n    openAnalyzer: false,\n  }),\n],\n\n// Or via CLI\n// npx webpack --analyze\n\n// Generate stats file for external tools\n// npx webpack --json > stats.json\n\n// Common optimizations:\noptimization: {\n  usedExports: true,      // Enable tree shaking\n  minimize: true,          // Enable minification\n  splitChunks: {           // Code splitting\n    chunks: \'all\',\n    maxSize: 244000,       // Target chunk size ~244KB\n  },\n},\nexternals: {\n  react: \'React\',         // Exclude React (use CDN)\n  \'react-dom\': \'ReactDOM\',\n},',
  followUp: 'What is the recommended maximum bundle size for initial page load, and how do you enforce bundle budgets in your build?',
  keyTakeaway: 'Use webpack-bundle-analyzer to visualize module sizes, then optimize by replacing heavy libraries, code-splitting, configuring externals, and verifying tree shaking effectiveness.',
},
{
  id: 28,
  title: 'What is the purpose of the externals configuration in Webpack?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'externals',
  answer: 'The externals configuration tells Webpack to exclude certain dependencies from the bundle and instead expect them to be available in the runtime environment as global variables, AMD modules, or CommonJS requires. This is commonly used in two scenarios: when building a library that should not include its peer dependencies (like React) in the output, and when deploying an application that loads certain libraries from a CDN. When you mark a dependency as external, Webpack replaces the import statement with a reference to the external global variable, dramatically reducing bundle size. The externals value can be a string (global variable name), an object mapping import names to global names, a function for dynamic resolution, or a regex to match multiple packages. For library authors, externals is essential to avoid bundling React, lodash, or other peer dependencies that the consuming application already provides. The externalsType property specifies the module format of the external (var, commonjs, amd, module, etc.).',
  codeExample: '// Application: load React from CDN\n// index.html\n// <script src="https://cdn.example.com/react.production.min.js"></script>\n\n// webpack.config.js\nexternals: {\n  // import React from \'react\' -> window.React\n  react: \'React\',\n  \'react-dom\': \'ReactDOM\',\n},\n\n// Library: exclude peer dependencies\nexternals: {\n  react: {\n    commonjs: \'react\',\n    commonjs2: \'react\',\n    amd: \'react\',\n    root: \'React\',\n  },\n},\n\n// Function form for dynamic externals\nexternals: [\n  function({ request }, callback) {\n    if (/^@company\\//.test(request)) {\n      return callback(null, \'commonjs \' + request);\n    }\n    callback();\n  },\n],',
  followUp: 'What are the risks of using CDN externals for critical dependencies like React in production?',
  keyTakeaway: 'Externals exclude specified dependencies from the bundle, expecting them to be provided by the runtime environment (CDN globals or host application), reducing bundle size for apps and libraries.',
},
{
  id: 29,
  title: 'How does Webpack handle dynamic imports?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'dynamic-imports',
  answer: 'When Webpack encounters an import() expression, it automatically creates a separate chunk for the imported module and its dependencies, then generates runtime code to load that chunk on demand via a JSONP-style script tag injection. At runtime, calling import() triggers a network request for the chunk file, and the returned Promise resolves with the module\'s exports once the script has loaded and executed. Webpack provides "magic comments" to control chunk behavior: webpackChunkName sets a human-readable chunk filename, webpackPrefetch adds a <link rel="prefetch"> hint for idle-time loading, webpackPreload adds <link rel="preload"> for parallel loading with the parent chunk, and webpackMode controls how Webpack resolves the import (lazy, eager, weak, lazy-once). When the import path contains an expression (e.g., import(`./locales/${lang}`)), Webpack creates a "context module" containing all possible matching files, each as a separate chunk. The output.publicPath configuration is critical for dynamic imports, as Webpack needs to know the base URL from which to load chunk files at runtime.',
  codeExample: '// Basic dynamic import - creates a separate chunk\nconst loadEditor = () => import(\'./Editor\');\n\n// Magic comments for optimization\nconst loadChart = () => import(\n  /* webpackChunkName: "chart-lib" */\n  /* webpackPrefetch: true */\n  \'./ChartLibrary\'\n);\n\n// webpackPreload: load in parallel with parent\nconst loadHero = () => import(\n  /* webpackPreload: true */\n  \'./HeroImage\'\n);\n\n// Expression-based dynamic import (context module)\nconst loadLocale = (lang) => import(\n  /* webpackChunkName: "locale-[request]" */\n  /* webpackMode: "lazy" */\n  `./locales/${lang}.json`\n);\n\n// webpackMode options:\n// "lazy"      - separate chunk per module (default)\n// "lazy-once" - single chunk for all matches\n// "eager"     - no extra chunk, included in parent\n// "weak"      - only works if already loaded',
  followUp: 'What is the difference between webpackPrefetch and webpackPreload, and when should you use each?',
  keyTakeaway: 'Webpack splits dynamic import() calls into separate chunks loaded on demand, with magic comments providing control over chunk naming, prefetching, preloading, and resolution mode.',
},
{
  id: 30,
  title: 'What is the DefinePlugin and how does it enable compile-time constants?',
  difficulty: 'medium',
  topic: 'webpack',
  subtopic: 'define-plugin',
  answer: 'DefinePlugin is a built-in Webpack plugin that performs compile-time text replacement of specified identifiers with their defined values throughout your source code. It works at the AST level, replacing every occurrence of a configured identifier with the literal value before any other processing, effectively creating compile-time constants. This is commonly used to inject environment-specific values like API URLs, feature flags, and the NODE_ENV variable that libraries like React use to strip development-only code in production builds. The values are code-stringified, meaning you typically need to use JSON.stringify() for string values, or they will be interpreted as code expressions. When combined with minification, DefinePlugin enables dead code elimination: if a condition like process.env.NODE_ENV === \'development\' evaluates to false at compile time, the minifier removes the entire unreachable code block. Webpack\'s mode: \'production\' automatically sets process.env.NODE_ENV to \'production\' via DefinePlugin.',
  codeExample: 'const webpack = require(\'webpack\');\n\nplugins: [\n  new webpack.DefinePlugin({\n    // Must use JSON.stringify for string values\n    \'process.env.API_URL\': JSON.stringify(\'https://api.example.com\'),\n    \'process.env.VERSION\': JSON.stringify(\'1.2.3\'),\n\n    // Boolean and number values work directly\n    __DEV__: JSON.stringify(true),\n    FEATURE_FLAGS: JSON.stringify({\n      newDashboard: true,\n      betaFeatures: false,\n    }),\n  }),\n],\n\n// In source code:\nif (__DEV__) {\n  console.log(\'Debug info\'); // Removed in production\n}\n\nfetch(`${process.env.API_URL}/users`);\n\n// After DefinePlugin (production):\nif (false) {\n  console.log(\'Debug info\'); // Dead code, minifier removes\n}\nfetch(\'https://api.example.com/users\');',
  followUp: 'What is the difference between DefinePlugin and EnvironmentPlugin, and when would you use each?',
  keyTakeaway: 'DefinePlugin replaces identifiers with literal values at compile time, enabling dead code elimination when the minifier removes unreachable branches created by constant conditions.',
},
{
  id: 31,
  title: 'How does Webpack\'s Hot Module Replacement work internally?',
  difficulty: 'hard',
  topic: 'webpack',
  subtopic: 'hmr-internals',
  answer: 'Webpack\'s HMR operates through a multi-step process involving the compiler, the dev server, and a client-side runtime. When a file changes, Webpack recompiles only the affected modules and generates an "update manifest" (a JSON file listing changed chunks) and "update chunks" (JavaScript files containing the new module code). The dev server sends a notification to the browser via WebSocket with the new compilation hash. The HMR client runtime in the browser receives this hash, fetches the update manifest to determine which chunks changed, then downloads the update chunks. The runtime then walks up the module dependency tree from the changed module, looking for "accept" handlers registered via module.hot.accept(). If an accept handler is found, the runtime replaces the old module with the new one and calls the handler, allowing the application to re-render with the updated code. If no accept handler is found all the way to the entry point, HMR falls back to a full page reload. Frameworks like React use React Fast Refresh to automatically register accept handlers for every component module, making HMR seamless for component updates without manual module.hot.accept() calls.',
  codeExample: '// HMR Internal Flow:\n// 1. File changes -> Webpack recompiles affected module\n// 2. Generates: hot-update.json (manifest) + hot-update.js (code)\n// 3. Dev server -> WebSocket -> browser: { hash: "abc123" }\n// 4. HMR runtime fetches manifest and update chunks\n// 5. Runtime walks dependency tree for accept handlers\n\n// Manual HMR accept handler\nif (module.hot) {\n  // Self-accepting module\n  module.hot.accept();\n\n  // Accept specific dependency changes\n  module.hot.accept(\'./renderer\', () => {\n    // Re-render with updated module\n    const { render } = require(\'./renderer\');\n    render();\n  });\n\n  // Cleanup before replacement\n  module.hot.dispose((data) => {\n    // Save state for next version\n    data.scrollPosition = window.scrollY;\n    clearInterval(myTimer);\n  });\n}\n\n// Update manifest (hot-update.json)\n// { "c": { "main": true }, "r": [], "m": [] }',
  followUp: 'How does React Fast Refresh handle state preservation during HMR for functional components versus class components?',
  keyTakeaway: 'HMR works by recompiling changed modules, sending update manifests via WebSocket, and walking the dependency tree for accept handlers that swap old modules with new ones at runtime.',
},
{
  id: 32,
  title: 'How would you write a custom Webpack loader?',
  difficulty: 'hard',
  topic: 'webpack',
  subtopic: 'custom-loader',
  answer: 'A Webpack loader is a JavaScript function that receives the source content of a file as its input and returns the transformed content. The function is exported as the default export of a Node.js module and receives the source as a string (or Buffer for raw loaders). The loader function has access to a context object (this) that provides utility methods: this.getOptions() retrieves loader options from the Webpack config, this.emitFile() generates additional output files, this.addDependency() marks files as dependencies for watch mode, and this.async() enables asynchronous loaders by returning a callback. Loaders should be stateless and deterministic, meaning the same input always produces the same output. For performance, loaders should use this.cacheable() (enabled by default in Webpack 5) to allow Webpack to cache the transformation result. You can chain loaders where each loader in the chain receives the previous loader\'s output; the last loader in the chain must return JavaScript or a module that Webpack can process. Loaders can also return source maps as a second argument to enable accurate debugging of the original source.',
  codeExample: '// markdown-loader.js - A custom loader that converts\n// markdown to a React component\nconst { marked } = require(\'marked\');\n\nmodule.exports = function markdownLoader(source) {\n  // Get loader options from webpack config\n  const options = this.getOptions();\n\n  // Convert markdown to HTML\n  const html = marked(source, options);\n\n  // Return a JavaScript module\n  return `\n    export default function MarkdownContent() {\n      return { __html: ${JSON.stringify(html)} };\n    }\n  `;\n};\n\n// Async loader example\nmodule.exports = function asyncLoader(source) {\n  const callback = this.async();\n\n  processAsync(source)\n    .then(result => callback(null, result))\n    .catch(err => callback(err));\n};\n\n// webpack.config.js - using the custom loader\nmodule: {\n  rules: [{\n    test: /\\.md$/,\n    use: [\n      {\n        loader: path.resolve(\'./markdown-loader.js\'),\n        options: { gfm: true },\n      },\n    ],\n  }],\n}',
  followUp: 'What is a pitching loader and how does the pitch phase differ from the normal loader execution phase?',
  keyTakeaway: 'A custom loader is a function that transforms file content, receiving source as input and returning JavaScript (or passing to the next loader), with access to options, async callbacks, and caching.',
},
{
  id: 33,
  title: 'How would you write a custom Webpack plugin using the tapable hooks system?',
  difficulty: 'hard',
  topic: 'webpack',
  subtopic: 'custom-plugin',
  answer: 'A Webpack plugin is a JavaScript class with an apply method that receives the compiler instance and uses Tapable hooks to tap into specific stages of the build lifecycle. The compiler object exposes hooks for the overall build process (compilation, emit, done, etc.), while the compilation object (accessible within compiler hooks) exposes hooks for individual build passes (buildModule, optimizeChunks, processAssets, etc.). You register your callback using tap (synchronous), tapAsync (callback-based async), or tapPromise (Promise-based async) on the desired hook. The processAssets hook in the compilation is commonly used to modify, add, or remove output files before they are written to disk. Each hook has a specific type (SyncHook, AsyncSeriesHook, AsyncParallelHook, etc.) that determines execution behavior. The plugin should provide a unique name in the tap call for debugging and hook ordering. Modern Webpack 5 plugins use the compilation.hooks.processAssets hook with Compilation.PROCESS_ASSETS_STAGE constants to control the order of asset processing.',
  codeExample: '// BannerPlugin-like custom plugin\nconst { Compilation } = require(\'webpack\');\n\nclass CustomBannerPlugin {\n  constructor(options) {\n    this.banner = options.banner;\n  }\n\n  apply(compiler) {\n    const pluginName = \'CustomBannerPlugin\';\n\n    compiler.hooks.compilation.tap(pluginName, (compilation) => {\n      compilation.hooks.processAssets.tap(\n        {\n          name: pluginName,\n          stage: Compilation.PROCESS_ASSETS_STAGE_ADDITIONS,\n        },\n        (assets) => {\n          for (const [name, source] of Object.entries(assets)) {\n            if (name.endsWith(\'.js\')) {\n              const banner = `/* ${this.banner} */\\n`;\n              compilation.updateAsset(\n                name,\n                new compiler.webpack.sources.ConcatSource(\n                  banner,\n                  source\n                )\n              );\n            }\n          }\n        }\n      );\n    });\n\n    // Async hook example\n    compiler.hooks.done.tapPromise(pluginName, async (stats) => {\n      console.log(`Build completed in ${stats.endTime - stats.startTime}ms`);\n    });\n  }\n}\n\n// Usage\nplugins: [new CustomBannerPlugin({ banner: \'Copyright 2026\' })]',
  followUp: 'What is the difference between compiler hooks and compilation hooks, and when would you use each?',
  keyTakeaway: 'A Webpack plugin is a class with an apply method that uses Tapable hooks (tap/tapAsync/tapPromise) on the compiler and compilation objects to intercept and modify the build at specific lifecycle stages.',
},
{
  id: 34,
  title: 'What is Module Federation\'s share scope and how does version negotiation work?',
  difficulty: 'hard',
  topic: 'webpack',
  subtopic: 'share-scope',
  answer: 'Module Federation\'s share scope is a runtime registry that coordinates which version of a shared dependency is loaded when multiple federated applications declare the same dependency. When applications configure shared dependencies, each one registers its available version in the share scope along with metadata: the version number, whether it is eager-loaded or lazy, the required version range, and whether it is a singleton (only one instance allowed). During runtime initialization, the share scope negotiation algorithm compares all registered versions and selects the highest compatible version that satisfies all consumers\' version requirements. If singleton mode is enabled (common for React), only one version loads globally; if a higher version is available from any remote, it is used, but if versions are incompatible, a warning is issued or an error thrown depending on strictVersion configuration. The requiredVersion field specifies the semver range each consumer accepts, and if no compatible version is found, the consumer falls back to loading its own bundled version. This negotiation happens asynchronously before any shared module is used, which is why Module Federation entry points must be async (typically achieved by having an async bootstrap file).',
  codeExample: '// App A: has React 18.2.0\nnew ModuleFederationPlugin({\n  name: \'appA\',\n  shared: {\n    react: {\n      singleton: true,         // Only one instance globally\n      requiredVersion: \'^18.0.0\',\n      version: \'18.2.0\',\n      eager: false,            // Lazy load (recommended)\n      strictVersion: false,    // Warn on mismatch, don\'t error\n    },\n  },\n});\n\n// App B: has React 18.3.0\nnew ModuleFederationPlugin({\n  name: \'appB\',\n  shared: {\n    react: {\n      singleton: true,\n      requiredVersion: \'^18.0.0\',\n      version: \'18.3.0\',\n    },\n  },\n});\n\n// Runtime negotiation result:\n// React 18.3.0 loaded (highest compatible version)\n// Both apps share the same React instance\n\n// Async bootstrap (required for share scope init)\n// bootstrap.js\nimport(\'./App\'); // Dynamic import ensures share scope loads first\n\n// index.js\nimport(\'./bootstrap\');',
  followUp: 'What happens when a singleton dependency has an incompatible version mismatch between two federated apps?',
  keyTakeaway: 'The share scope is a runtime registry where federated apps negotiate shared dependency versions, selecting the highest compatible version and enforcing singleton constraints to prevent duplicate instances.',
},
{
  id: 35,
  title: 'How does Webpack 5\'s persistent caching work and what are its trade-offs?',
  difficulty: 'hard',
  topic: 'webpack',
  subtopic: 'persistent-cache',
  answer: 'Webpack 5\'s persistent caching stores the results of the entire compilation (resolved modules, generated code, source maps, and intermediate artifacts) to the filesystem between builds, dramatically reducing subsequent build times by reusing work from previous compilations. When cache.type is set to "filesystem", Webpack serializes its internal data structures to disk (default location: node_modules/.cache/webpack) and on the next build, deserializes and validates the cache entries against their dependencies. Cache invalidation is managed through build dependencies (webpack config files, loaders, plugins) and snapshot strategies that track file timestamps and content hashes. The buildDependencies.config option tells Webpack which files, when changed, should invalidate the entire cache (typically your webpack config file). You can configure the snapshot strategy via snapshot.module and snapshot.resolve to use timestamp-based (faster, less accurate) or content-hash-based (slower, fully accurate) validation. The trade-offs include: disk space usage (caches can grow to hundreds of MB), potential for stale caches if invalidation is misconfigured (especially with custom loaders or plugins that have implicit dependencies), and the initial overhead of serializing the cache on first build.',
  codeExample: '// webpack.config.js - Persistent caching\nmodule.exports = {\n  cache: {\n    type: \'filesystem\',\n\n    // Cache location\n    cacheDirectory: path.resolve(__dirname, \'.webpack-cache\'),\n\n    // Files that invalidate entire cache when changed\n    buildDependencies: {\n      config: [__filename],         // This config file\n      tsconfig: [\'./tsconfig.json\'], // TypeScript config\n    },\n\n    // Cache version - bump to invalidate manually\n    version: \'1.0\',\n\n    // Name for the cache (useful for multi-config)\n    name: `${process.env.NODE_ENV}-cache`,\n  },\n\n  // Snapshot strategy\n  snapshot: {\n    module: {\n      timestamp: true,   // Check file timestamps\n      hash: true,        // Also verify content hash\n    },\n    resolve: {\n      timestamp: true,\n    },\n    // node_modules assumed immutable by default\n    managedPaths: [path.resolve(__dirname, \'node_modules\')],\n  },\n};\n\n// Build time comparison:\n// First build:  ~45 seconds (+ cache serialization)\n// Second build: ~5 seconds  (cache hit)\n// After config change: ~45 seconds (cache invalidated)',
  followUp: 'How do you handle cache invalidation for custom loaders that depend on external state or files not tracked by Webpack?',
  keyTakeaway: 'Persistent caching serializes compilation artifacts to disk for near-instant rebuilds, but requires careful configuration of build dependencies and snapshot strategies to prevent stale cache issues.',
},
{
  id: 36,
  title: 'What is Vite and how does it differ from Webpack?',
  difficulty: 'easy',
  topic: 'vite',
  subtopic: 'esm-dev',
  answer: 'Vite is a next-generation frontend build tool created by Evan You that leverages native ES modules in the browser during development. Unlike Webpack, which bundles your entire application before serving it, Vite serves source files over native ESM and only transforms them on-demand as the browser requests them. In production, Vite uses Rollup for optimized bundling with tree shaking and code splitting. This architecture means Vite\'s dev server starts almost instantly regardless of application size, whereas Webpack\'s startup time grows linearly with the number of modules. Vite also provides built-in support for TypeScript, JSX, CSS Modules, and many other features without requiring additional loaders or configuration.',
  codeExample: '// vite.config.ts  minimal config\nimport { defineConfig } from \'vite\'\nimport react from \'@vitejs/plugin-react\'\n\nexport default defineConfig({\n  plugins: [react()],\n})',
  followUp: 'When would you still choose Webpack over Vite for a new project?',
  keyTakeaway: 'Vite achieves fast dev starts by serving native ES modules on demand instead of bundling the entire app upfront.',
},
{
  id: 37,
  title: 'Why is Vite\'s dev server so fast compared to traditional bundlers?',
  difficulty: 'easy',
  topic: 'vite',
  subtopic: 'esm-dev',
  answer: 'Vite\'s dev server is fast because it avoids bundling your application code entirely during development. Instead, it serves each module as a separate HTTP request using the browser\'s native ES module support, transforming files on-demand only when the browser imports them. Dependencies from node_modules are pre-bundled once using esbuild (which is 10-100x faster than JavaScript-based tools) and cached aggressively. When you edit a file, only that specific module and its direct dependents are invalidated via HMR, rather than rebuilding large chunks of the dependency graph. This combination of native ESM serving, esbuild-powered pre-bundling, and granular HMR makes Vite\'s dev experience nearly instantaneous.',
  followUp: 'What happens when a Vite project grows to thousands of modules  does the dev server slow down?',
  keyTakeaway: 'Vite skips bundling your source code during development and relies on native ESM plus esbuild pre-bundling for near-instant startup.',
},
{
  id: 38,
  title: 'What is dependency pre-bundling in Vite?',
  difficulty: 'easy',
  topic: 'vite',
  subtopic: 'pre-bundling',
  answer: 'Dependency pre-bundling is the process where Vite uses esbuild to convert node_modules dependencies into optimized ESM bundles before serving them. This is necessary for two reasons: first, many npm packages ship as CommonJS which browsers cannot natively import, so they must be converted to ESM. Second, packages like lodash-es contain hundreds of internal modules, and importing them individually would trigger hundreds of HTTP requests, degrading browser performance. Pre-bundling collapses these into a single module per dependency. The results are cached in node_modules/.vite and only re-run when dependencies change. You can force a re-bundle by deleting this cache directory or running the dev server with the --force flag.',
  codeExample: '// vite.config.ts  customizing pre-bundling\nexport default defineConfig({\n  optimizeDeps: {\n    include: [\'linked-dep\'],  // force pre-bundle\n    exclude: [\'my-esm-pkg\'], // skip pre-bundling\n  },\n})',
  followUp: 'How would you troubleshoot a dependency that fails during pre-bundling?',
  keyTakeaway: 'Vite pre-bundles node_modules with esbuild to convert CJS to ESM and collapse many internal modules into single files.',
},
{
  id: 39,
  title: 'How does Vite handle CSS and CSS Modules?',
  difficulty: 'easy',
  topic: 'vite',
  subtopic: 'css-handling',
  answer: 'Vite provides built-in support for CSS without any plugins. Importing a .css file injects its content into the page via a <style> tag during development and extracts it into separate CSS files during production builds. CSS Modules are supported automatically for any file ending in .module.css  the imported object contains the locally scoped class name mappings. Vite also supports PostCSS (if a postcss.config.js is present), Sass, Less, and Stylus with just the preprocessor dependency installed. During development, CSS changes trigger instant HMR updates without a full page reload, and in production, CSS is minified and code-split alongside the JavaScript chunks.',
  codeExample: '// Component.module.css\n.header { color: red; }\n\n// Component.tsx\nimport styles from \'./Component.module.css\'\n// styles.header => \'_header_x7kd2_1\' (scoped name)',
  followUp: 'How does Vite handle CSS code splitting in production builds?',
  keyTakeaway: 'Vite natively supports CSS, CSS Modules, preprocessors, and PostCSS with zero configuration and instant HMR.',
},
{
  id: 40,
  title: 'How does Vite use native ES modules during development?',
  difficulty: 'medium',
  topic: 'vite',
  subtopic: 'esm-dev',
  answer: 'During development, Vite acts as an HTTP server that intercepts module requests from the browser. When your index.html contains a <script type="module">, the browser fetches the entry module, parses its import statements, and sends additional requests for each dependency. Vite intercepts these requests, transforms the file on-the-fly (handling TypeScript, JSX, CSS, etc.), rewrites bare import specifiers like "react" to pre-bundled paths like /node_modules/.vite/deps/react.js, and serves the result with proper HTTP caching headers. This means only the modules actually used on the current page are loaded, and subsequent requests are served from the browser\'s HTTP cache with 304 responses. The browser\'s native module loading effectively replaces the bundler\'s module graph traversal.',
  codeExample: '// What you write:\nimport { useState } from \'react\'\nimport App from \'./App\'\n\n// What the browser receives from Vite:\nimport { useState } from \'/node_modules/.vite/deps/react.js?v=abc123\'\nimport App from \'/src/App.tsx?t=1234567890\'',
  followUp: 'How does Vite handle dynamic imports and code splitting in dev mode versus production?',
  keyTakeaway: 'Vite rewrites import specifiers and transforms files on-demand, letting the browser\'s native ESM loader drive module resolution.',
},
{
  id: 41,
  title: 'What is the difference between Vite\'s dev mode and production build?',
  difficulty: 'medium',
  topic: 'vite',
  subtopic: 'production-build',
  answer: 'Vite uses fundamentally different strategies for development and production. In dev mode, Vite serves unbundled ES modules over a fast dev server, transforming files on-demand with esbuild for TypeScript and JSX compilation. In production, Vite delegates bundling to Rollup, which performs thorough tree shaking, code splitting, chunk optimization, and asset processing. The reason for this split is that unbundled ESM in production would result in hundreds of HTTP requests and poor performance, even with HTTP/2. Rollup\'s production output is minified, has stable chunk hashing for long-term caching, and can produce multiple output formats. This dual-engine approach gives developers the fastest possible DX while maintaining optimal production performance.',
  codeExample: '// vite.config.ts  production-specific config\nexport default defineConfig({\n  build: {\n    rollupOptions: {\n      output: {\n        manualChunks: {\n          vendor: [\'react\', \'react-dom\'],\n        },\n      },\n    },\n    minify: \'terser\',       // or \'esbuild\' (default)\n    sourcemap: true,\n    target: \'es2015\',\n  },\n})',
  followUp: 'Why did Vite choose Rollup over esbuild for production builds?',
  keyTakeaway: 'Vite uses native ESM with esbuild for fast development and Rollup for optimized, tree-shaken production bundles.',
},
{
  id: 42,
  title: 'What is the Vite plugin API and how does it extend Rollup\'s plugin system?',
  difficulty: 'medium',
  topic: 'vite',
  subtopic: 'plugin-api',
  answer: 'Vite\'s plugin API is a superset of Rollup\'s plugin interface, meaning any Rollup plugin can be used in Vite with minimal changes. Vite extends Rollup\'s hooks with additional dev-server-specific hooks like configureServer (for adding custom middleware), transformIndexHtml (for modifying the HTML entry), and handleHotUpdate (for custom HMR logic). Plugins can specify when they should run using the enforce property  "pre" runs before Vite core transforms, and "post" runs after. Plugins can also conditionally apply to only dev or build using the apply property. This design lets the ecosystem share plugins between Vite and Rollup while giving Vite plugins access to dev-time features that Rollup doesn\'t need.',
  codeExample: '// A Vite plugin skeleton\nexport function myPlugin(): Plugin {\n  return {\n    name: \'my-plugin\',\n    enforce: \'pre\',           // run before core plugins\n    apply: \'serve\',           // only during dev\n    configureServer(server) {\n      server.middlewares.use((req, res, next) => {\n        // custom middleware\n        next()\n      })\n    },\n    transform(code, id) {\n      if (id.endsWith(\'.custom\')) {\n        return { code: transformCustom(code) }\n      }\n    },\n  }\n}',
  followUp: 'How would you write a Vite plugin that injects environment-specific HTML tags?',
  keyTakeaway: 'Vite\'s plugin API extends Rollup\'s plugin interface with dev-server hooks like configureServer and handleHotUpdate.',
},
{
  id: 43,
  title: 'How does Vite\'s dependency pre-bundling work with esbuild under the hood?',
  difficulty: 'medium',
  topic: 'vite',
  subtopic: 'pre-bundling',
  answer: 'When Vite starts the dev server, it scans your source code for bare imports (like "react" or "lodash-es") using esbuild\'s fast parser to discover all dependencies. It then runs esbuild with these discovered entry points, bundling each dependency and its internal imports into a single flat ESM file stored in node_modules/.vite/deps. esbuild handles CJS-to-ESM conversion, resolves internal package imports, and produces browser-compatible output in milliseconds. Vite generates a metadata file (_metadata.json) tracking dependency versions and configuration hashes to determine when re-bundling is needed. If you add a new dependency during dev, Vite detects the new import and triggers a re-optimization automatically, prompting a page reload. This approach leverages esbuild\'s Go-based parallelism to pre-bundle hundreds of dependencies in under a second.',
  followUp: 'What happens if a dependency has side effects that esbuild\'s pre-bundling might affect?',
  keyTakeaway: 'Vite scans source imports and uses esbuild to flatten each dependency into a single ESM file cached in node_modules/.vite/deps.',
},
{
  id: 44,
  title: 'How do you configure Vite for a library build using library mode?',
  difficulty: 'medium',
  topic: 'vite',
  subtopic: 'library-mode',
  answer: 'Vite\'s library mode allows you to bundle a package for distribution by setting build.lib in the config. You specify the entry point and the library name (used for UMD/IIFE global variable), and Vite will produce optimized bundles using Rollup. By default, it outputs both ES module and UMD formats, though you can customize formats to include CJS as well. External dependencies like React should be listed in build.rollupOptions.external so they are not bundled into the output. Vite will also generate proper export maps and handle CSS extraction. Library mode skips HTML processing and focuses on producing clean, distributable packages with proper module exports.',
  codeExample: '// vite.config.ts  library mode\nimport { resolve } from \'path\'\nimport { defineConfig } from \'vite\'\n\nexport default defineConfig({\n  build: {\n    lib: {\n      entry: resolve(__dirname, \'src/index.ts\'),\n      name: \'MyLib\',\n      formats: [\'es\', \'cjs\', \'umd\'],\n      fileName: (format) => `my-lib.${format}.js`,\n    },\n    rollupOptions: {\n      external: [\'react\', \'react-dom\'],\n      output: {\n        globals: {\n          react: \'React\',\n          \'react-dom\': \'ReactDOM\',\n        },\n      },\n    },\n  },\n})',
  followUp: 'How do you handle CSS extraction and TypeScript declarations when publishing a Vite library?',
  keyTakeaway: 'Vite\'s library mode uses build.lib to produce distributable ES, CJS, and UMD bundles with Rollup under the hood.',
},
{
  id: 45,
  title: 'What is import.meta.hot and how does Vite\'s HMR API work?',
  difficulty: 'medium',
  topic: 'vite',
  subtopic: 'hmr-api',
  answer: 'import.meta.hot is Vite\'s HMR (Hot Module Replacement) API object, available only during development. It allows modules to accept updates to themselves or their dependencies without a full page reload. The primary method is import.meta.hot.accept(), which registers a callback that runs when the module or its dependencies are updated. You can use accept() with no arguments for self-accepting modules, or pass dependency paths to handle updates to specific imports. The API also provides dispose() for cleanup when a module is replaced, invalidate() to propagate updates up the module graph, and prune() for cleanup when a module is removed. Framework plugins like @vitejs/plugin-react handle HMR automatically for components, so you rarely need to use this API directly unless building frameworks or custom modules.',
  codeExample: '// Custom HMR handling\nif (import.meta.hot) {\n  // Self-accepting: module handles its own updates\n  import.meta.hot.accept((newModule) => {\n    if (newModule) {\n      render(newModule.default)\n    }\n  })\n\n  // Cleanup on module replacement\n  import.meta.hot.dispose(() => {\n    clearInterval(timer)\n  })\n\n  // Force update propagation\n  import.meta.hot.invalidate()\n}',
  followUp: 'How would you implement HMR for a custom templating language in Vite?',
  keyTakeaway: 'import.meta.hot provides accept, dispose, and invalidate methods for fine-grained control over hot module replacement in Vite.',
},
{
  id: 46,
  title: 'How does Vite handle environment variables and .env files?',
  difficulty: 'medium',
  topic: 'vite',
  subtopic: 'env-variables',
  answer: 'Vite loads environment variables from .env files in your project root using the dotenv library. It supports .env, .env.local, .env.development, .env.production, and .env.[mode] files, with mode-specific files taking priority over generic ones. For security, only variables prefixed with VITE_ are exposed to your client-side code via import.meta.env  this prevents accidentally leaking server secrets to the browser bundle. Built-in variables include import.meta.env.MODE, import.meta.env.BASE_URL, import.meta.env.PROD, and import.meta.env.DEV. During the build, these references are statically replaced with their values, enabling dead code elimination. You can customize the prefix from VITE_ to something else using the envPrefix config option, and get TypeScript support by declaring types in an env.d.ts file.',
  codeExample: '// .env.development\nVITE_API_URL=http://localhost:3001\nVITE_APP_TITLE=My App (Dev)\nSECRET_KEY=not-exposed  // NOT available in client code\n\n// src/env.d.ts  TypeScript support\ninterface ImportMetaEnv {\n  readonly VITE_API_URL: string\n  readonly VITE_APP_TITLE: string\n}\n\n// Usage in code\nconsole.log(import.meta.env.VITE_API_URL)',
  followUp: 'How would you handle environment variables that need to differ between staging and production?',
  keyTakeaway: 'Vite exposes only VITE_-prefixed variables from .env files to client code via import.meta.env for security.',
},
{
  id: 47,
  title: 'How does Vite handle SSR (Server-Side Rendering) and what is ssrLoadModule?',
  difficulty: 'hard',
  topic: 'vite',
  subtopic: 'ssr',
  answer: 'Vite provides first-class SSR support through its dev server API. The key method is server.ssrLoadModule(url), which loads a module through Vite\'s transform pipeline and returns it in a Node.js-compatible format without bundling. This means your server-side code benefits from the same HMR, TypeScript support, and plugin transforms as client code during development. Vite automatically externalizes node_modules during SSR to avoid unnecessary transformation of server dependencies. For production SSR, you run a separate build with ssr: true in the config, which produces a Node.js-compatible bundle. The ssrFixStacktrace function helps map error stack traces back to original source files. Vite\'s SSR support is framework-agnostic and powers meta-frameworks like Nuxt, SolidStart, and Astro.',
  codeExample: '// server.js  SSR with Vite dev server\nimport { createServer } from \'vite\'\n\nconst vite = await createServer({\n  server: { middlewareMode: true },\n  appType: \'custom\',\n})\n\napp.use(vite.middlewares)\n\napp.use(\'*\', async (req, res) => {\n  const url = req.originalUrl\n  let template = await vite.transformIndexHtml(url, html)\n  const { render } = await vite.ssrLoadModule(\'/src/entry-server.ts\')\n  const appHtml = await render(url)\n  res.send(template.replace(\'<!--ssr-outlet-->\', appHtml))\n})',
  followUp: 'How does Vite handle CSS during SSR to prevent flash of unstyled content?',
  keyTakeaway: 'Vite\'s ssrLoadModule transforms and loads server-side modules through the same plugin pipeline as client code without bundling.',
},
{
  id: 48,
  title: 'What are the limitations of Vite\'s ESM-based dev server compared to bundled dev servers?',
  difficulty: 'hard',
  topic: 'vite',
  subtopic: 'esm-dev',
  answer: 'While Vite\'s native ESM approach is extremely fast for startup, it has several trade-offs. The biggest limitation is the waterfall problem: when the browser encounters an import, it must fetch that module before discovering its imports, leading to sequential request chains that can slow initial page loads on large module graphs. This is less of an issue with HTTP/2 multiplexing and warm caches, but can be noticeable on first load of deep dependency chains. Some npm packages still don\'t work well with ESM, requiring manual optimizeDeps configuration. Proxy and CORS configurations can interact poorly with the many HTTP requests in ESM mode. Browser compatibility is another factor  older browsers that don\'t support ESM need a separate legacy build. Finally, the dev and production environments use fundamentally different bundling strategies, which can occasionally cause bugs that only appear in one mode.',
  followUp: 'How does Vite mitigate the waterfall problem for deeply nested module imports?',
  keyTakeaway: 'Vite\'s unbundled ESM dev server can suffer from request waterfalls on deep import chains and dev/prod behavior differences.',
},
{
  id: 49,
  title: 'How does Vite\'s module graph work internally for dependency tracking?',
  difficulty: 'hard',
  topic: 'vite',
  subtopic: 'module-graph',
  answer: 'Vite maintains an internal ModuleGraph data structure that tracks every module loaded during development and its import relationships. Each module is represented as a ModuleNode containing the URL, file path, transform result, importers (who imports this module), and importedModules (what this module imports). When a file changes, Vite walks the module graph upward through importers to determine the HMR boundary  the nearest module that accepts hot updates. If no boundary is found, a full page reload is triggered. The module graph is populated lazily as the browser requests modules and is updated after each transform. Plugins can access the module graph via server.moduleGraph to query dependencies, invalidate modules with invalidateModule(), or retrieve transformed content. This graph is also what enables Vite\'s efficient CSS HMR, as it knows exactly which components import which stylesheets.',
  codeExample: '// Accessing the module graph in a plugin\nconfigureServer(server) {\n  server.middlewares.use(async (req, res, next) => {\n    const module = server.moduleGraph.getModuleByUrl(\'/src/App.tsx\')\n    if (module) {\n      console.log(\'Importers:\', [...module.importers])\n      console.log(\'Imports:\', [...module.importedModules])\n      // Manually invalidate for re-transform\n      server.moduleGraph.invalidateModule(module)\n    }\n    next()\n  })\n}',
  followUp: 'How would you use the module graph API to build a custom dependency visualization tool?',
  keyTakeaway: 'Vite\'s ModuleGraph tracks import relationships between all loaded modules to determine precise HMR boundaries and invalidation.',
},
{
  id: 50,
  title: 'How would you write a Vite plugin that transforms custom file types?',
  difficulty: 'hard',
  topic: 'vite',
  subtopic: 'plugin-api',
  answer: 'To handle custom file types in Vite, you write a plugin that implements the resolveId, load, and transform hooks. The resolveId hook tells Vite how to resolve imports of your custom extension. The load hook reads the file and returns initial content, and the transform hook converts it into valid JavaScript or CSS that the browser can consume. You should filter by file extension in the transform hook using the id parameter. For HMR support, implement the handleHotUpdate hook to define how changes to your custom files propagate. The plugin should return code with proper source maps for debugging. Since Vite plugins are a superset of Rollup plugins, the same plugin will also work during the production Rollup build without changes.',
  codeExample: '// vite-plugin-markdown.ts\nimport { Plugin } from \'vite\'\nimport { marked } from \'marked\'\n\nexport function markdown(): Plugin {\n  return {\n    name: \'vite-plugin-markdown\',\n    transform(code, id) {\n      if (!id.endsWith(\'.md\')) return null\n\n      const html = marked(code)\n      return {\n        code: `export default ${JSON.stringify(html)}`,\n        map: null,\n      }\n    },\n    handleHotUpdate({ file, server }) {\n      if (file.endsWith(\'.md\')) {\n        const mod = server.moduleGraph.getModuleByUrl(file)\n        if (mod) return [mod] // trigger HMR for this module\n      }\n    },\n  }\n}',
  followUp: 'How would you add source map support to a custom file type transform plugin?',
  keyTakeaway: 'Custom Vite plugins use transform to convert non-JS files into valid JavaScript and handleHotUpdate for HMR integration.',
},
{
  id: 51,
  title: 'What is Rollup and what is it best suited for?',
  difficulty: 'easy',
  topic: 'rollup',
  subtopic: 'library-bundling',
  answer: 'Rollup is a JavaScript module bundler designed with an ESM-first philosophy. It was created by Rich Harris and is best suited for bundling libraries and packages because of its superior tree shaking and ability to produce clean, minimal output in multiple formats. Unlike Webpack, which was designed for applications with features like dev servers and HMR, Rollup focuses on producing the smallest possible bundle by treating every module as ES module and eliminating dead code through static analysis. Rollup powers the production builds of Vite, and many popular libraries like React, Vue, Svelte, and D3 use Rollup for their published builds. Its output is generally more readable and smaller than Webpack\'s because it hoists modules into a single scope rather than wrapping each in a function.',
  followUp: 'Why is Rollup\'s output often more readable than Webpack\'s?',
  keyTakeaway: 'Rollup is an ESM-first bundler best suited for library bundling due to its superior tree shaking and clean output.',
},
{
  id: 52,
  title: 'What output formats does Rollup support?',
  difficulty: 'easy',
  topic: 'rollup',
  subtopic: 'output-formats',
  answer: 'Rollup supports five output formats: es (ES modules with import/export), cjs (CommonJS with require/module.exports), umd (Universal Module Definition that works in browsers, Node.js, and AMD), iife (Immediately Invoked Function Expression for direct browser <script> tags), and amd (Asynchronous Module Definition for RequireJS). The es format is ideal for modern bundlers and tree shaking, cjs for Node.js compatibility, and umd for universal distribution. You can output multiple formats from a single build by providing an array of output configurations. Each format can have its own file naming, directory, and plugin configuration, making Rollup excellent for publishing packages that need to support different module systems.',
  codeExample: '// rollup.config.js  multiple output formats\nexport default {\n  input: \'src/index.js\',\n  output: [\n    { file: \'dist/bundle.esm.js\', format: \'es\' },\n    { file: \'dist/bundle.cjs.js\', format: \'cjs\' },\n    { file: \'dist/bundle.umd.js\', format: \'umd\', name: \'MyLib\' },\n  ],\n}',
  followUp: 'When would you choose the IIFE format over UMD?',
  keyTakeaway: 'Rollup supports es, cjs, umd, iife, and amd output formats, and can produce all of them in a single build.',
},
{
  id: 53,
  title: 'How do you configure Rollup to bundle a simple library?',
  difficulty: 'easy',
  topic: 'rollup',
  subtopic: 'library-bundling',
  answer: 'A basic Rollup library configuration requires an input entry point, an output configuration specifying the file path and format, and typically a few plugins for resolving node_modules and handling CommonJS dependencies. The @rollup/plugin-node-resolve plugin allows Rollup to find third-party modules in node_modules, while @rollup/plugin-commonjs converts CommonJS modules to ESM so Rollup can process them. For TypeScript projects, you would add @rollup/plugin-typescript or rollup-plugin-typescript2. External dependencies that consumers will provide (like React) should be listed in the external array so they are not bundled. The configuration file is typically named rollup.config.js and is itself an ES module.',
  codeExample: '// rollup.config.js\nimport resolve from \'@rollup/plugin-node-resolve\'\nimport commonjs from \'@rollup/plugin-commonjs\'\nimport typescript from \'@rollup/plugin-typescript\'\n\nexport default {\n  input: \'src/index.ts\',\n  output: {\n    file: \'dist/index.esm.js\',\n    format: \'es\',\n    sourcemap: true,\n  },\n  external: [\'react\', \'react-dom\'],\n  plugins: [resolve(), commonjs(), typescript()],\n}',
  followUp: 'How do you handle peer dependencies versus bundled dependencies in a Rollup library config?',
  keyTakeaway: 'A Rollup library config needs an entry point, output format, node-resolve and commonjs plugins, and external declarations for peer dependencies.',
},
{
  id: 54,
  title: 'How does Rollup\'s tree shaking compare to Webpack\'s?',
  difficulty: 'medium',
  topic: 'rollup',
  subtopic: 'tree-shaking',
  answer: 'Rollup\'s tree shaking is generally more effective than Webpack\'s because of its ESM-first architecture and scope hoisting. Rollup treats all code as ES modules from the start, allowing it to perform static analysis on import/export statements to determine exactly which exports are used. It then eliminates unused exports and any code only reachable through those exports. Webpack historically bundled everything and then tried to remove unused code, which was less precise. Rollup also performs scope hoisting (placing all modules in a single scope) by default, which makes dead code elimination more straightforward because the minifier can see the entire call graph without function wrapper boundaries. However, Webpack 5 has significantly improved its tree shaking with its own scope hoisting implementation, narrowing the gap considerably.',
  followUp: 'How does the sideEffects field in package.json affect tree shaking in Rollup versus Webpack?',
  keyTakeaway: 'Rollup\'s tree shaking is more effective because its ESM-first design and scope hoisting let it statically analyze the full module graph.',
},
{
  id: 55,
  title: 'What are Rollup\'s plugin hooks and the build lifecycle?',
  difficulty: 'medium',
  topic: 'rollup',
  subtopic: 'plugin-hooks',
  answer: 'Rollup\'s build lifecycle is divided into two phases: the build phase and the output generation phase. During the build phase, hooks like options, buildStart, resolveId, load, transform, and buildEnd are called sequentially as Rollup processes each module. resolveId determines how import paths map to files, load reads the module content, and transform modifies the code (e.g., transpiling TypeScript). During the output generation phase, hooks like renderStart, banner, footer, renderChunk, augmentChunkHash, and generateBundle are called to finalize the output. Hooks can be async, first (stops after first non-null result), sequential (runs in plugin order), or parallel (runs all concurrently). Understanding this lifecycle is essential for writing plugins that intercept and transform code at the right stage.',
  codeExample: '// Rollup build lifecycle (simplified order)\n// BUILD PHASE:\n//   options -> buildStart\n//   resolveId -> load -> transform (per module)\n//   moduleParsed (per module)\n//   buildEnd\n//\n// OUTPUT PHASE:\n//   renderStart\n//   renderChunk (per chunk)\n//   generateBundle\n//   writeBundle (if writing to disk)\n//   closeBundle',
  followUp: 'What is the difference between a "first" hook and a "sequential" hook in Rollup?',
  keyTakeaway: 'Rollup\'s plugin lifecycle has build-phase hooks (resolveId, load, transform) and output-phase hooks (renderChunk, generateBundle).',
},
{
  id: 56,
  title: 'How do you configure Rollup for a library that outputs both CJS and ESM?',
  difficulty: 'medium',
  topic: 'rollup',
  subtopic: 'library-bundling',
  answer: 'To output both CJS and ESM from Rollup, you provide an array of output configurations, each specifying its format, file path, and any format-specific settings. The ESM output uses format "es" and typically has a .mjs or .esm.js extension, while CJS uses format "cjs" with a .cjs or .cjs.js extension. You should also set the exports field in your package.json to point consumers to the correct format using the "exports" map with "import" and "require" conditions. For CJS output, you may need to set output.exports to "named" or "auto" to control how exports are generated. Both formats share the same input and plugins, so you only need one build pass.',
  codeExample: '// rollup.config.js  dual CJS + ESM output\nexport default {\n  input: \'src/index.ts\',\n  external: [\'react\'],\n  plugins: [resolve(), commonjs(), typescript()],\n  output: [\n    {\n      file: \'dist/index.mjs\',\n      format: \'es\',\n      sourcemap: true,\n    },\n    {\n      file: \'dist/index.cjs\',\n      format: \'cjs\',\n      exports: \'named\',\n      sourcemap: true,\n    },\n  ],\n}\n\n// package.json exports map\n// "exports": {\n//   ".": {\n//     "import": "./dist/index.mjs",\n//     "require": "./dist/index.cjs"\n//   }\n// }',
  followUp: 'How do you handle default exports differently between CJS and ESM output?',
  keyTakeaway: 'Rollup produces dual CJS/ESM output via an output array, paired with package.json exports map for correct consumer resolution.',
},
{
  id: 57,
  title: 'What is the difference between Rollup\'s resolveId, load, and transform hooks?',
  difficulty: 'medium',
  topic: 'rollup',
  subtopic: 'plugin-hooks',
  answer: 'These three hooks form the core module processing pipeline in Rollup. resolveId maps an import specifier (like "./utils" or "lodash") to an absolute file path or virtual module ID  it answers "where is this module?" The load hook takes that resolved ID and returns the module\'s source code  it answers "what is the content of this module?" This is where virtual modules (modules that don\'t exist on disk) provide their content. The transform hook receives the loaded source code and transforms it  it answers "how should this code be modified?" This is where TypeScript compilation, JSX transformation, or any other code manipulation happens. Each hook runs in plugin order, and returning null passes control to the next plugin or Rollup\'s default behavior.',
  codeExample: '// Plugin demonstrating all three hooks\nexport function myPlugin() {\n  return {\n    name: \'my-plugin\',\n    resolveId(source) {\n      if (source === \'virtual:config\') return \'\\0virtual:config\'\n      return null // let other plugins/Rollup handle it\n    },\n    load(id) {\n      if (id === \'\\0virtual:config\') {\n        return \'export const ENV = \"production\"\' // virtual module content\n      }\n      return null // let Rollup read from disk\n    },\n    transform(code, id) {\n      if (id.endsWith(\'.svg\')) {\n        return `export default ${JSON.stringify(code)}`\n      }\n      return null // no transformation\n    },\n  }\n}',
  followUp: 'Why do virtual module IDs conventionally start with \\0 in Rollup plugins?',
  keyTakeaway: 'resolveId maps import paths to IDs, load returns module content from those IDs, and transform modifies the loaded code.',
},
{
  id: 58,
  title: 'How does Rollup handle external dependencies?',
  difficulty: 'medium',
  topic: 'rollup',
  subtopic: 'external-deps',
  answer: 'External dependencies in Rollup are modules that should not be included in the bundle, instead remaining as import/require statements in the output. You declare them using the external option, which accepts an array of module names, a regex pattern, or a function that receives the module ID and returns true to externalize it. This is crucial for library authors who want consumers to provide their own copy of peer dependencies like React or lodash. For UMD/IIFE output formats, externalized modules need corresponding entries in the output.globals map to specify the global variable name they\'ll be available as. Rollup will warn if it encounters an unresolved dependency that isn\'t marked as external, helping catch configuration issues early.',
  codeExample: '// rollup.config.js  external dependencies\nexport default {\n  input: \'src/index.ts\',\n  external: [\n    \'react\',\n    \'react-dom\',\n    /^@mui\\//,            // regex: externalize all @mui packages\n    (id) => id.includes(\'node_modules\'),  // function form\n  ],\n  output: {\n    format: \'umd\',\n    name: \'MyLib\',\n    globals: {\n      react: \'React\',\n      \'react-dom\': \'ReactDOM\',\n    },\n  },\n}',
  followUp: 'What happens when you forget to externalize a peer dependency in a library bundle?',
  keyTakeaway: 'Rollup\'s external option excludes dependencies from the bundle, leaving them as import statements for consumers to resolve.',
},
{
  id: 59,
  title: 'What are Rollup\'s chunk naming and manual chunks strategies?',
  difficulty: 'medium',
  topic: 'rollup',
  subtopic: 'chunk-naming',
  answer: 'When Rollup performs code splitting (via dynamic imports or multiple entry points), it creates chunks that need naming strategies. The output.chunkFileNames option controls how generated chunks are named using placeholders like [name], [hash], and [format]  for example, "chunks/[name]-[hash].js". The output.manualChunks option gives you explicit control over which modules go into which chunks. It can be an object mapping chunk names to arrays of module IDs, or a function that receives a module ID and returns a chunk name. This is useful for creating a vendor chunk with all node_modules, splitting large dependencies into separate chunks for better caching, or grouping related modules together. Proper chunk naming with content hashes enables long-term browser caching.',
  codeExample: '// rollup.config.js  chunk strategies\nexport default {\n  input: [\'src/main.js\', \'src/admin.js\'],\n  output: {\n    dir: \'dist\',\n    format: \'es\',\n    chunkFileNames: \'chunks/[name]-[hash].js\',\n    manualChunks(id) {\n      if (id.includes(\'node_modules\')) {\n        if (id.includes(\'react\')) return \'vendor-react\'\n        return \'vendor\'\n      }\n    },\n  },\n}',
  followUp: 'How do you prevent Rollup from creating too many small chunks that increase HTTP overhead?',
  keyTakeaway: 'Rollup\'s manualChunks option lets you explicitly control code splitting for optimal caching and chunk size strategies.',
},
{
  id: 60,
  title: 'How does Rollup achieve superior tree shaking through its ESM-first architecture?',
  difficulty: 'hard',
  topic: 'rollup',
  subtopic: 'side-effect-analysis',
  answer: 'Rollup\'s tree shaking advantage comes from treating the entire module graph as a single scope through a technique called scope hoisting (or module concatenation). Instead of wrapping each module in a function like Webpack traditionally does, Rollup inlines modules into a flat scope, renaming variables to avoid conflicts. This gives Rollup and subsequent minifiers complete visibility into which functions and variables are actually referenced. Rollup performs side effect analysis on each statement  it tracks whether a function call might produce side effects, whether a variable assignment is ever read, and whether an import is used for its value or only its side effects. Modules marked with "sideEffects": false in package.json can be eliminated entirely if none of their exports are used. Rollup also understands that ES module imports are live bindings and static, which makes its analysis mathematically precise compared to the dynamic nature of CommonJS require.',
  followUp: 'How does Rollup handle tree shaking for re-exported modules in barrel files?',
  keyTakeaway: 'Rollup\'s scope hoisting flattens all modules into a single scope, enabling precise dead code elimination through complete static analysis.',
},
{
  id: 61,
  title: 'How would you write a Rollup plugin from scratch?',
  difficulty: 'hard',
  topic: 'rollup',
  subtopic: 'virtual-modules',
  answer: 'A Rollup plugin is a function that returns an object with a name property and one or more hook functions. The most common pattern involves resolveId, load, and transform hooks for processing modules, plus generateBundle for emitting additional files. Each hook receives specific arguments and can return a value to influence Rollup\'s behavior or null to defer to other plugins. The plugin function typically accepts an options parameter for configuration. For virtual modules (modules that don\'t exist on disk), you resolve them with a \\0 prefix convention to prevent other plugins from trying to load them as files. Plugins can maintain internal state, read the module graph via the this context, and emit warnings or errors through this.warn() and this.error(). Testing plugins typically involves creating a minimal Rollup build programmatically using the rollup.rollup() API.',
  codeExample: '// rollup-plugin-json-import.js\nexport function jsonImport(options = {}) {\n  const filter = options.include || /\\.json$/\n\n  return {\n    name: \'json-import\',\n\n    transform(code, id) {\n      if (!filter.test(id)) return null\n\n      try {\n        const parsed = JSON.parse(code)\n        const entries = Object.entries(parsed)\n        const exports = entries\n          .map(([key, val]) =>\n            `export const ${key} = ${JSON.stringify(val)}`\n          ).join(\'\\n\')\n\n        return {\n          code: `${exports}\\nexport default ${code}`,\n          map: { mappings: \'\' },\n        }\n      } catch (err) {\n        this.error(`Failed to parse JSON: ${err.message}`)\n      }\n    },\n  }\n}',
  followUp: 'How would you add caching to a Rollup plugin to avoid redundant transforms?',
  keyTakeaway: 'A Rollup plugin is a named object with hook functions like resolveId, load, transform, and generateBundle that intercept the build process.',
},
{
  id: 62,
  title: 'How does Rollup handle circular dependencies differently from Webpack?',
  difficulty: 'hard',
  topic: 'rollup',
  subtopic: 'circular-deps',
  answer: 'Rollup handles circular dependencies by leveraging the live binding nature of ES modules. When two modules import from each other, Rollup hoists them into the same scope and orders the declarations so that variables are defined before they are used. Because ES module imports are live bindings (references, not copies), a variable imported before initialization will eventually resolve to the correct value once the exporting module\'s code executes. Rollup warns about circular dependencies by default because they can lead to undefined references if the execution order doesn\'t match expectations. Webpack\'s CommonJS approach is different  require() returns the partially-executed exports object at the time of the call, which can result in getting an empty or incomplete object. Rollup\'s approach is generally more predictable for ES modules but can still produce runtime errors if a circularly imported value is accessed during module evaluation rather than lazily.',
  codeExample: '// a.js  circular dependency example\nimport { b } from \'./b.js\'\nexport const a = \'a\'\nexport function getB() { return b } // works: lazy access\n\n// b.js\nimport { a } from \'./a.js\'\nexport const b = \'b\'\nconsole.log(a) // may be undefined if a.js hasn\'t executed yet\n\n// Rollup output (simplified, hoisted into one scope):\nconst a = \'a\'\nconst b = \'b\'\nconsole.log(a) // works because of scope hoisting order',
  followUp: 'How would you detect and refactor circular dependencies in a large codebase?',
  keyTakeaway: 'Rollup uses ES module live bindings and scope hoisting to resolve circular dependencies more predictably than CommonJS-based bundlers.',
},
{
  id: 63,
  title: 'What is esbuild and why is it significantly faster than JavaScript-based bundlers?',
  difficulty: 'easy',
  topic: 'esbuild',
  subtopic: 'go-architecture',
  answer: 'esbuild is a JavaScript and TypeScript bundler and minifier written in Go by Evan Wallace (co-founder of Figma). It is 10-100x faster than JavaScript-based bundlers like Webpack and Rollup because of three key architectural decisions. First, it is written in Go, a compiled language with native code performance, whereas JavaScript bundlers run in a single-threaded interpreted V8 runtime. Second, esbuild heavily parallelizes work across all available CPU cores using Go\'s goroutines for parsing, linking, and code generation simultaneously. Third, it minimizes memory allocations and data passes  esbuild processes the entire bundle in very few passes over the AST, reducing overhead from garbage collection and memory copying. These design choices make esbuild capable of bundling large projects in milliseconds rather than seconds.',
  followUp: 'If esbuild is so fast, why don\'t all tools use it as their primary bundler?',
  keyTakeaway: 'esbuild achieves 10-100x speed gains by using Go\'s native compilation, parallelism across CPU cores, and minimal memory allocation.',
},
{
  id: 64,
  title: 'What are esbuild\'s main use cases?',
  difficulty: 'easy',
  topic: 'esbuild',
  subtopic: 'use-cases',
  answer: 'esbuild excels in several primary use cases. It is most commonly used as a fast TypeScript and JSX transpiler, stripping types and transforming syntax without performing full type checking. Vite uses esbuild for dependency pre-bundling during development, converting node_modules from CJS to ESM in milliseconds. esbuild is also popular as a fast minifier  tools like Vite use it as the default minifier instead of Terser for significantly faster production builds. It works well for bundling serverless functions and CLI tools where build speed matters and advanced features like HMR are unnecessary. Additionally, esbuild can bundle CSS, handle assets, and produce source maps. However, it is generally not used as the sole production bundler for large web applications due to its limited code splitting and plugin ecosystem compared to Webpack or Rollup.',
  followUp: 'Why is esbuild often used as a component within other tools rather than as a standalone bundler?',
  keyTakeaway: 'esbuild is primarily used for fast transpilation, dependency pre-bundling, minification, and bundling serverless/CLI tools.',
},
{
  id: 65,
  title: 'How do you use esbuild to bundle a simple project?',
  difficulty: 'easy',
  topic: 'esbuild',
  subtopic: 'build-api',
  answer: 'esbuild can be used via its CLI or its JavaScript/Go API. The CLI approach is the simplest  you specify an entry point, output path, and options like bundle mode and target format. The JavaScript API provides the same functionality programmatically through the build() function, which accepts a configuration object. Key options include entryPoints (input files), outfile or outdir (output location), bundle (whether to inline imports), format (esm, cjs, or iife), platform (browser or node), and target (the JavaScript version to compile down to). esbuild automatically handles TypeScript and JSX without additional plugins. The build() function returns a promise with metadata about the output, including any warnings or errors encountered during the build.',
  codeExample: '// CLI usage\n// npx esbuild src/index.ts --bundle --outfile=dist/out.js --format=esm\n\n// JavaScript API\nimport { build } from \'esbuild\'\n\nawait build({\n  entryPoints: [\'src/index.ts\'],\n  bundle: true,\n  outfile: \'dist/out.js\',\n  format: \'esm\',\n  platform: \'browser\',\n  target: \'es2020\',\n  minify: true,\n  sourcemap: true,\n})',
  followUp: 'How does esbuild\'s watch mode compare to Webpack\'s watch mode in terms of rebuild speed?',
  keyTakeaway: 'esbuild bundles projects via a CLI or JavaScript API with options for format, platform, target, minification, and source maps.',
},
{
  id: 66,
  title: 'How does esbuild handle TypeScript files?',
  difficulty: 'easy',
  topic: 'esbuild',
  subtopic: 'typescript',
  answer: 'esbuild handles TypeScript by performing syntax stripping  it removes all type annotations, interfaces, type imports, and other TypeScript-specific syntax to produce valid JavaScript. Crucially, esbuild does NOT perform type checking; it only transforms the syntax. This is a deliberate design decision that contributes to esbuild\'s speed, as type checking requires resolving the full type graph across all files, which is inherently slower. esbuild supports most TypeScript syntax including enums, decorators (with the right config), JSX in .tsx files, and path aliases configured through tsconfig.json. However, some TypeScript features that require type information to emit correct code, like const enums across files and certain decorator metadata, are not fully supported. For type safety, you should run tsc --noEmit separately as part of your CI or build pipeline.',
  codeExample: '// esbuild TypeScript config\nimport { build } from \'esbuild\'\n\nawait build({\n  entryPoints: [\'src/index.ts\'],\n  bundle: true,\n  outfile: \'dist/out.js\',\n  tsconfig: \'./tsconfig.json\',\n  // TypeScript-specific options:\n  // jsx is auto-detected from .tsx extension\n  // decorators supported via tsconfig experimentalDecorators\n})\n\n// Run type checking separately:\n// tsc --noEmit',
  followUp: 'What TypeScript features does esbuild not support and how do you work around them?',
  keyTakeaway: 'esbuild strips TypeScript types without type checking for maximum speed  run tsc --noEmit separately for type safety.',
},
{
  id: 67,
  title: 'How does esbuild\'s architecture differ from Webpack and Rollup?',
  difficulty: 'medium',
  topic: 'esbuild',
  subtopic: 'go-architecture',
  answer: 'esbuild\'s architecture fundamentally differs from JavaScript-based bundlers in its execution model. Webpack and Rollup run on Node.js, which means single-threaded JavaScript execution with JIT compilation overhead, garbage collection pauses, and the overhead of the V8 runtime. esbuild is compiled directly to native machine code from Go, eliminating interpreter overhead entirely. esbuild parallelizes three main phases  parsing, linking, and code generation  across all CPU cores using Go goroutines, whereas Webpack and Rollup process modules largely sequentially. esbuild also uses a shared AST representation across all phases, avoiding the serialize/deserialize overhead that JavaScript-based tools incur when passing data between plugins and build stages. This means esbuild makes far fewer memory allocations and almost no copies of the AST, while Webpack\'s plugin architecture inherently requires multiple transformations and serializations of the module graph.',
  followUp: 'How does esbuild communicate between its Go core and the JavaScript plugin API?',
  keyTakeaway: 'esbuild compiles to native code, parallelizes across all CPU cores, and shares a single AST in memory instead of serializing between stages.',
},
{
  id: 68,
  title: 'What are esbuild\'s limitations compared to Webpack?',
  difficulty: 'medium',
  topic: 'esbuild',
  subtopic: 'limitations',
  answer: 'esbuild has several significant limitations compared to Webpack. Its plugin API is intentionally limited  plugins can only intercept resolve and load operations, whereas Webpack offers dozens of hooks throughout the compilation lifecycle. esbuild has no native HMR (Hot Module Replacement) support, making it unsuitable as a standalone dev server for web applications. Code splitting in esbuild is more basic, supporting only ES module output with dynamic imports and lacking Webpack\'s sophisticated chunk splitting strategies. esbuild does not perform type checking for TypeScript. Its CSS handling, while functional, lacks the extensive PostCSS ecosystem and CSS-in-JS support that Webpack provides through loaders. esbuild also cannot produce certain output formats like AMD, and its configuration options for advanced scenarios like Module Federation or Web Workers are limited compared to Webpack\'s mature ecosystem.',
  followUp: 'How do tools like Vite work around esbuild\'s limitations while still leveraging its speed?',
  keyTakeaway: 'esbuild lacks HMR, has limited plugin hooks, basic code splitting, no type checking, and fewer output format options compared to Webpack.',
},
{
  id: 69,
  title: 'What is the esbuild plugin API and what are its constraints?',
  difficulty: 'medium',
  topic: 'esbuild',
  subtopic: 'plugin-api',
  answer: 'esbuild\'s plugin API provides two main hooks: onResolve and onLoad. The onResolve hook intercepts module resolution  you register a filter (regex) and a callback that determines how an import path maps to a file or virtual module. The onLoad hook intercepts module loading  given a resolved path, it returns the module\'s contents and loader type. These two hooks are intentionally the only extension points because adding more hooks would require crossing the Go-JavaScript boundary more frequently, negating esbuild\'s speed advantage. Each Go-to-JavaScript bridge call involves serialization overhead, so esbuild minimizes these crossings. Plugins run in the JavaScript process and communicate with esbuild\'s Go core through stdin/stdout or IPC. This means plugins themselves can be slow (since they run in Node.js), and having many plugins that match many files can significantly reduce esbuild\'s speed advantages.',
  codeExample: '// esbuild plugin example\nimport { build } from \'esbuild\'\n\nconst envPlugin = {\n  name: \'env-plugin\',\n  setup(build) {\n    // Intercept imports starting with "env:"\n    build.onResolve({ filter: /^env:/ }, (args) => ({\n      path: args.path.slice(4),\n      namespace: \'env\',\n    }))\n\n    // Provide virtual module content\n    build.onLoad({ filter: /.*/, namespace: \'env\' }, (args) => ({\n      contents: `export default ${JSON.stringify(process.env[args.path])}`,\n      loader: \'js\',\n    }))\n  },\n}\n\nawait build({\n  entryPoints: [\'src/index.ts\'],\n  bundle: true,\n  plugins: [envPlugin],\n})',
  followUp: 'How does the performance of an esbuild plugin compare to a Webpack loader for the same task?',
  keyTakeaway: 'esbuild\'s plugin API is limited to onResolve and onLoad hooks to minimize Go-JavaScript boundary crossings and preserve speed.',
},
{
  id: 70,
  title: 'How do you use esbuild for bundling a Node.js application?',
  difficulty: 'medium',
  topic: 'esbuild',
  subtopic: 'node-bundling',
  answer: 'To bundle a Node.js application with esbuild, you set platform to "node" which adjusts several behaviors automatically. It marks built-in Node.js modules (fs, path, http, etc.) as external so they are not bundled, sets the default output format to CJS (since Node historically uses CommonJS), and adjusts module resolution to follow Node\'s algorithm. You can also set external to exclude specific node_modules that should remain as require() calls  this is useful for native addons (.node files) or packages that don\'t bundle well. Setting format to "esm" produces ES module output for Node.js versions that support it. The target option should be set to your minimum Node version (like "node18") so esbuild only transforms syntax unsupported by that version. This approach is popular for bundling serverless functions, CLI tools, and microservices into single files for faster cold starts.',
  codeExample: '// esbuild Node.js bundling\nimport { build } from \'esbuild\'\n\nawait build({\n  entryPoints: [\'src/server.ts\'],\n  bundle: true,\n  outfile: \'dist/server.js\',\n  platform: \'node\',\n  target: \'node18\',\n  format: \'esm\',\n  // Exclude native modules and problematic packages\n  external: [\'better-sqlite3\', \'sharp\'],\n  // Replace __dirname for ESM output\n  banner: {\n    js: \'import { createRequire } from "module"; const require = createRequire(import.meta.url);\',\n  },\n})',
  followUp: 'What challenges arise when bundling a Node.js app that uses native C++ addons?',
  keyTakeaway: 'Setting platform to "node" auto-externalizes built-in modules and configures Node-compatible resolution for bundling servers and CLI tools.',
},
{
  id: 71,
  title: 'How does esbuild handle CSS bundling and CSS modules?',
  difficulty: 'medium',
  topic: 'esbuild',
  subtopic: 'css-bundling',
  answer: 'esbuild provides native CSS bundling support. When you import a CSS file from JavaScript, esbuild processes it as a separate entry point and produces a corresponding CSS output file alongside the JavaScript bundle. esbuild resolves @import statements, inlines them into the output, and handles url() references by copying or inlining assets based on the loader configuration. For CSS Modules, esbuild supports .module.css files starting from version 0.16, generating locally scoped class names and exporting the name mapping to JavaScript. esbuild can also minify CSS, removing whitespace, shortening color values, and merging duplicate rules. However, esbuild\'s CSS handling is more basic than PostCSS  it does not support custom PostCSS plugins, Sass/Less preprocessing, or advanced features like CSS nesting in older output targets without a polyfill.',
  codeExample: '// esbuild CSS bundling\nawait build({\n  entryPoints: [\'src/app.tsx\'],\n  bundle: true,\n  outdir: \'dist\',\n  // CSS is automatically extracted to dist/app.css\n  // CSS Modules: .module.css files get scoped class names\n  loader: {\n    \'.png\': \'file\',   // copy referenced images\n    \'.svg\': \'text\',   // inline SVGs as strings\n  },\n})\n\n// In your code:\nimport styles from \'./Button.module.css\'\n// styles.primary => scoped class name',
  followUp: 'How would you integrate PostCSS with esbuild for features like autoprefixer?',
  keyTakeaway: 'esbuild natively bundles CSS with @import resolution, CSS Modules support, and minification, but lacks PostCSS plugin integration.',
},
{
  id: 72,
  title: 'Why did esbuild choose Go instead of Rust, and how does this affect its performance model?',
  difficulty: 'hard',
  topic: 'esbuild',
  subtopic: 'go-architecture',
  answer: 'Evan Wallace chose Go for esbuild primarily because of Go\'s simplicity, fast compilation times, and excellent built-in concurrency primitives (goroutines and channels). Go compiles to native code nearly instantly, which made development iteration faster than Rust\'s slower compiler. Go\'s goroutines are lightweight green threads managed by the Go runtime, making it trivial to parallelize parsing, linking, and code generation across thousands of concurrent tasks without manual thread management. Rust would theoretically offer slightly better raw performance through zero-cost abstractions and no garbage collector, but esbuild\'s architecture minimizes GC pressure by using arenas and reducing allocations, making the GC overhead negligible. The trade-off is that Go has a garbage collector that can occasionally pause execution and Go binaries are somewhat larger. In practice, both Rust-based tools (like SWC) and Go-based esbuild achieve similar order-of-magnitude speedups over JavaScript-based tools, suggesting that the language choice matters less than the architectural decisions.',
  followUp: 'How does esbuild minimize garbage collection pauses, and what arena allocation patterns does it use?',
  keyTakeaway: 'Go was chosen for fast compilation, simple goroutine parallelism, and rapid development velocity  the GC overhead is minimized through allocation-conscious design.',
},
{
  id: 73,
  title: 'What trade-offs did esbuild make for speed (no type checking, limited plugin hooks)?',
  difficulty: 'hard',
  topic: 'esbuild',
  subtopic: 'speed',
  answer: 'esbuild made several deliberate trade-offs to achieve maximum speed. First, it performs no TypeScript type checking, which would require building and traversing a complete type graph across all files  an inherently slow operation that tsc itself struggles with. Second, its plugin API is limited to only resolve and load hooks, because each hook invocation crosses the Go-JavaScript process boundary with serialization overhead. More hooks would mean more boundary crossings, degrading performance. Third, esbuild does not support HMR because implementing it would add complexity to the core architecture and slow down the bundling pipeline. Fourth, esbuild\'s code splitting is simpler than Webpack\'s  it doesn\'t support advanced strategies like Module Federation or sophisticated chunk optimization. Fifth, esbuild does not guarantee output stability across versions, prioritizing speed optimizations over backward-compatible output. These trade-offs are why esbuild is often used as a component within larger tools (like Vite) rather than as a standalone build system for complex applications.',
  followUp: 'If esbuild added full TypeScript type checking, how much would it slow down and why?',
  keyTakeaway: 'esbuild sacrifices type checking, rich plugin hooks, HMR, and advanced code splitting to minimize Go-JS boundary crossings and maximize throughput.',
},
{
  id: 74,
  title: 'How would you integrate esbuild into an existing Webpack project for faster transpilation?',
  difficulty: 'hard',
  topic: 'esbuild',
  subtopic: 'integration',
  answer: 'The most common approach is to replace Webpack\'s babel-loader or ts-loader with esbuild-loader, which uses esbuild as the transpiler within Webpack\'s existing build pipeline. This gives you esbuild\'s fast transpilation speed while keeping Webpack\'s full feature set (HMR, code splitting, plugin ecosystem). You can also use esbuild as the minifier by replacing TerserPlugin with ESBuildMinifyPlugin, which is often the single biggest speed improvement since minification is typically the most time-consuming build step. The migration is incremental  you can swap the loader for TypeScript and JSX files while keeping other loaders unchanged. Be aware that esbuild-loader does not support all Babel plugins (like custom syntax transforms or emotion\'s CSS-in-JS transform), so some projects may need to keep Babel for specific files while using esbuild for the majority. This hybrid approach can reduce build times by 50-80% without any architectural changes.',
  codeExample: '// webpack.config.js  integrating esbuild\nconst { EsbuildPlugin } = require(\'esbuild-loader\')\n\nmodule.exports = {\n  module: {\n    rules: [\n      {\n        test: /\\.[jt]sx?$/,\n        loader: \'esbuild-loader\',\n        options: {\n          target: \'es2020\',\n          // jsx: \'automatic\', // for React 17+ JSX transform\n        },\n      },\n    ],\n  },\n  optimization: {\n    minimizer: [\n      new EsbuildPlugin({\n        target: \'es2020\',\n        css: true,  // also minify CSS\n      }),\n    ],\n  },\n}',
  followUp: 'What Babel plugins or features would you lose by switching to esbuild-loader, and how would you handle them?',
  keyTakeaway: 'Replace babel-loader with esbuild-loader and TerserPlugin with EsbuildPlugin for 50-80% faster Webpack builds without architectural changes.',
},
{
  id: 75,
  title: 'What is Parcel and what is its core philosophy?',
  difficulty: 'easy',
  topic: 'parcel',
  subtopic: 'zero-config',
  answer: 'Parcel is a zero-configuration web application bundler that aims to make the build process as simple as possible. Its core philosophy is that developers should be able to point Parcel at an entry file and get a production-ready bundle without writing any configuration. Parcel automatically detects the project structure, file types, and required transformations by analyzing the dependency graph starting from the entry point. It supports HTML, CSS, JavaScript, TypeScript, images, and many other asset types out of the box. Parcel 2 was rewritten with a plugin-based architecture and Rust-based internals for significantly improved performance.',
  followUp: 'How does Parcel\'s zero-config approach handle edge cases where customization is needed?',
  keyTakeaway: 'Parcel is a zero-configuration bundler that automatically detects project structure and asset types to produce optimized bundles without manual setup.',
},
{
  id: 76,
  title: 'How does Parcel achieve zero configuration?',
  difficulty: 'easy',
  topic: 'parcel',
  subtopic: 'zero-config',
  answer: 'Parcel achieves zero configuration by using sensible defaults and auto-detection of project settings. It reads existing configuration files like .babelrc, tsconfig.json, .postcssrc, and browserslist to determine transformation requirements. When these files are absent, Parcel applies reasonable defaults such as transpiling modern JavaScript for broad browser support. The resolver automatically handles node_modules, file extensions, and package.json fields like "main", "module", and "browser". Parcel also auto-installs missing dependencies when it encounters an import that is not yet installed, further reducing friction in the development workflow.',
  followUp: 'What happens when Parcel\'s defaults conflict with a project\'s specific requirements?',
  keyTakeaway: 'Parcel auto-detects project configuration from existing config files and applies sensible defaults when they are absent, eliminating the need for a bundler config file.',
},
{
  id: 77,
  title: 'How does Parcel handle different asset types (images, fonts, CSS, HTML)?',
  difficulty: 'easy',
  topic: 'parcel',
  subtopic: 'resolver',
  answer: 'Parcel treats every file as an asset in its dependency graph, not just JavaScript modules. When Parcel encounters an import for an image, font, or other non-JS file, it processes the asset through the appropriate transformer pipeline and emits it with a content hash in the output directory. CSS files are automatically processed with PostCSS and support CSS modules, nesting, and vendor prefixing out of the box. HTML files can serve as entry points, and Parcel will discover all referenced scripts, stylesheets, images, and other assets by parsing the HTML. This unified asset model means you can import a PNG from JavaScript and receive the final URL, or reference a font in CSS, and Parcel handles the resolution and optimization automatically.',
  codeExample: '// Importing assets in JavaScript\\nimport heroImage from \'./hero.png\'\\nimport styles from \'./styles.module.css\'\\n\\n// In HTML entry point\\n// <link rel="stylesheet" href="./styles.css" />\\n// <img src="./hero.png" />\\n// Parcel resolves and bundles all of these automatically',
  followUp: 'How does Parcel optimize image assets during the build process?',
  keyTakeaway: 'Parcel treats every file type as a first-class asset, automatically resolving, transforming, and emitting images, fonts, CSS, and HTML without extra configuration.',
},
{
  id: 78,
  title: 'How does Parcel\'s automatic code splitting work?',
  difficulty: 'medium',
  topic: 'parcel',
  subtopic: 'auto-code-splitting',
  answer: 'Parcel performs automatic code splitting by analyzing dynamic import() expressions and shared dependencies in the asset graph. When Parcel encounters a dynamic import, it creates a new bundle for the imported module and its unique dependencies. Parcel also detects modules that are shared between multiple bundles and automatically extracts them into common bundles to avoid duplication. Unlike Webpack, which requires manual configuration of splitChunks, Parcel\'s code splitting is fully automatic and produces optimized bundles based on the actual dependency graph. Parcel uses heuristics to determine when extracting a shared module is worthwhile, considering factors like module size and the number of bundles that reference it.',
  codeExample: '// Dynamic import triggers automatic code splitting\\nconst Dashboard = () => {\\n  const [module, setModule] = useState(null)\\n\\n  useEffect(() => {\\n    // Parcel creates a separate bundle for Chart\\n    import(\'./Chart\').then(m => setModule(m))\\n  }, [])\\n\\n  return module ? <module.Chart /> : <Loading />\\n}\\n\\n// Shared deps between routes are auto-extracted\\n// No splitChunks config needed',
  followUp: 'How does Parcel decide when a shared module is worth extracting into a separate bundle?',
  keyTakeaway: 'Parcel automatically splits code at dynamic import boundaries and extracts shared dependencies into common bundles without any manual configuration.',
},
{
  id: 79,
  title: 'What is Parcel\'s asset graph and how does it differ from Webpack\'s module graph?',
  difficulty: 'medium',
  topic: 'parcel',
  subtopic: 'asset-graph',
  answer: 'Parcel\'s asset graph is a unified representation of all assets and their dependencies in a project, where every file type is a first-class node in the graph. Unlike Webpack\'s module graph which is primarily JavaScript-centric and requires loaders to convert non-JS files into modules, Parcel\'s asset graph natively understands HTML, CSS, images, and other file types as interconnected assets. Each asset in the graph can have multiple output typesfor example, a CSS file imported from JavaScript creates both a CSS bundle and a JavaScript reference to it. The asset graph drives bundling, code splitting, and optimization decisions holistically across all asset types. This design allows Parcel to perform cross-asset optimizations like automatically inlining small images or eliminating unused CSS that would require separate plugins in Webpack.',
  followUp: 'How does the asset graph handle circular dependencies between different asset types?',
  keyTakeaway: 'Parcel\'s asset graph treats all file types as first-class nodes, enabling cross-asset optimization that Webpack\'s JavaScript-centric module graph requires extra plugins to achieve.',
},
{
  id: 80,
  title: 'What are Parcel transformers and how do they process assets?',
  difficulty: 'medium',
  topic: 'parcel',
  subtopic: 'transformers',
  answer: 'Parcel transformers are plugins responsible for parsing and transforming individual assets in the build pipeline. Each asset type is matched to a transformer based on file extension or glob patterns defined in the .parcelrc configuration file. Transformers receive a raw asset and produce one or more transformed assets along with any discovered dependencies. For example, the JavaScript transformer parses the code into an AST, extracts import statements as dependencies, and can apply transformations like JSX compilation or TypeScript stripping. Multiple transformers can be chained in a pipelinea .ts file might pass through the TypeScript transformer and then the JavaScript transformer. Parcel ships with built-in transformers for common types and allows custom transformers to be registered for specialized asset processing.',
  codeExample: '// .parcelrc - configuring transformer pipelines\\n{\\n  "extends": "@parcel/config-default",\\n  "transformers": {\\n    "*.svg": ["@parcel/transformer-svg-react"],\\n    "*.{ts,tsx}": [\\n      "@parcel/transformer-typescript-tsc",\\n      "@parcel/transformer-js"\\n    ]\\n  }\\n}',
  followUp: 'How do Parcel transformers communicate discovered dependencies back to the asset graph?',
  keyTakeaway: 'Parcel transformers are chainable plugins that parse, transform, and extract dependencies from individual assets, with each file type matched to a specific transformer pipeline.',
},
{
  id: 81,
  title: 'How does Parcel\'s scope hoisting optimize output bundles?',
  difficulty: 'medium',
  topic: 'parcel',
  subtopic: 'scope-hoisting',
  answer: 'Parcel\'s scope hoisting concatenates modules into a single scope rather than wrapping each module in a separate function closure, which is the approach used in development mode. This optimization reduces the number of function calls and closures in the output, resulting in smaller bundle sizes and faster execution at runtime. Scope hoisting enables dead code elimination because unused exports become unreferenced variables that minifiers can safely remove. Parcel performs scope hoisting by analyzing the module graph and renaming variables to avoid conflicts when merging modules into shared scopes. When a module has side effects or uses patterns incompatible with static analysis (such as eval or module.exports reassignment), Parcel falls back to wrapping that specific module while still hoisting the rest.',
  followUp: 'What module patterns prevent Parcel from applying scope hoisting and how can they be refactored?',
  keyTakeaway: 'Scope hoisting merges modules into a single scope instead of wrapping each in a closure, reducing bundle size and enabling better dead code elimination.',
},
{
  id: 82,
  title: 'What is Parcel\'s content hashing strategy and how does it enable long-term caching?',
  difficulty: 'medium',
  topic: 'parcel',
  subtopic: 'content-hashing',
  answer: 'Parcel generates content hashes based on the actual contents of each output bundle and includes them in the output filenames. When a file\'s content changes, its hash changes, which produces a new filename and forces browsers to download the updated version. Files whose content has not changed retain the same hash and filename, allowing them to be served from the browser cache indefinitely. Parcel uses a cascading hash strategy where a bundle\'s hash depends only on its own content and the hashes of its referenced bundles, not on unrelated bundles. This means changing code in one module only invalidates the caches of bundles that actually include that code. The approach enables setting far-future cache headers (Cache-Control: max-age=31536000) on all hashed assets for optimal long-term caching.',
  followUp: 'How does Parcel handle content hashing for assets referenced in CSS url() declarations?',
  keyTakeaway: 'Parcel uses content-based hashing in filenames so only bundles with actual content changes get new URLs, enabling aggressive long-term browser caching.',
},
{
  id: 83,
  title: 'How does Parcel 2\'s architecture with its Rust-based transformer pipeline achieve parallelism?',
  difficulty: 'hard',
  topic: 'parcel',
  subtopic: 'rust-pipeline',
  answer: 'Parcel 2 uses a Rust-based core through the @parcel/rust package and native N-API bindings to perform CPU-intensive operations like JavaScript parsing, scope analysis, and dependency collection outside the JavaScript main thread. The architecture leverages Rust\'s native threading model to process multiple assets in parallel across all available CPU cores, unlike JavaScript-based bundlers that are constrained by Node.js\'s single-threaded event loop. Parcel\'s asset graph construction is incrementalwhen a file changes, only the affected portion of the graph is reprocessed while unaffected assets retain their cached results. The Rust transformer uses SWC under the hood for JavaScript and TypeScript parsing, which provides near-native performance for AST operations. Communication between the JavaScript plugin layer and the Rust core happens through efficient serialization, minimizing the overhead of crossing the language boundary. This hybrid architecture allows Parcel to maintain its extensible JavaScript plugin ecosystem while achieving the raw performance of systems-level code for the hot paths.',
  followUp: 'What are the trade-offs of Parcel\'s hybrid Rust-JavaScript architecture compared to a fully Rust-based bundler like Rspack?',
  keyTakeaway: 'Parcel 2 uses Rust-based internals with native threading to parallelize asset transformation across all CPU cores while maintaining a JavaScript plugin ecosystem.',
},
{
  id: 84,
  title: 'How does Parcel\'s automatic differential bundling for modern vs legacy browsers work?',
  difficulty: 'hard',
  topic: 'parcel',
  subtopic: 'targets',
  answer: 'Parcel\'s differential bundling automatically produces separate bundles for modern and legacy browsers based on the browserslist configuration in the project. When an HTML entry point is used, Parcel generates modern ES module bundles loaded with <script type="module"> and legacy bundles loaded with <script nomodule>, ensuring each browser downloads only the code it needs. The modern bundles contain native async/await, arrow functions, and other ES2017+ syntax, resulting in significantly smaller payloads. Legacy bundles include all necessary polyfills and transpiled code for older browsers. Parcel handles this entirely automatically without requiring multiple configuration entries or separate build passesit compiles both targets in a single build by forking the asset graph at the output stage. This approach can reduce payload size for modern browsers by 20-40% compared to shipping a single legacy-compatible bundle to all browsers.',
  codeExample: '<!-- Parcel automatically generates this HTML output -->\\n<script type="module" src="/app.modern.a1b2c3.js"></script>\\n<script nomodule src="/app.legacy.d4e5f6.js"></script>\\n\\n<!-- browserslist in package.json drives the split -->\\n// "browserslist": {\\n//   "modern": "last 2 Chrome versions, last 2 Firefox versions",\\n//   "legacy": "> 0.5%, not dead"\\n// }',
  followUp: 'How does differential bundling interact with Parcel\'s code splitting to avoid a combinatorial explosion of bundles?',
  keyTakeaway: 'Parcel automatically generates separate modern and legacy bundles using module/nomodule script tags, reducing payload for modern browsers without extra configuration.',
},
{
  id: 85,
  title: 'What is Turbopack and what problem does it solve?',
  difficulty: 'easy',
  topic: 'turbopack',
  subtopic: 'rust-architecture',
  answer: 'Turbopack is a Rust-based bundler created by Vercel that is designed to be the successor to Webpack, particularly for large-scale applications where build performance is critical. It solves the problem of slow development builds and hot module replacement in large codebases by leveraging Rust\'s performance characteristics and an incremental computation architecture. Turbopack was built from the ground up to handle the scale of modern monorepos and enterprise applications where Webpack\'s JavaScript-based architecture becomes a bottleneck. It is developed by the same team that created Webpack (Tobias Koppers joined Vercel) and draws on lessons learned from Webpack\'s design over the past decade. Turbopack is integrated into Next.js as its development bundler starting from Next.js 13.',
  followUp: 'Why was Rust chosen as the language for Turbopack instead of using JavaScript or Go?',
  keyTakeaway: 'Turbopack is a Rust-based bundler by Vercel designed to replace Webpack for large applications where JavaScript-based bundlers become too slow.',
},
{
  id: 86,
  title: 'How is Turbopack related to Next.js?',
  difficulty: 'easy',
  topic: 'turbopack',
  subtopic: 'nextjs-integration',
  answer: 'Turbopack is the default development bundler for Next.js starting from Next.js 15, replacing Webpack for local development. You enable it in earlier versions with the --turbopack flag when running next dev. Turbopack is deeply integrated with Next.js and understands its conventions like the App Router, Server Components, and server actions natively. The integration means Turbopack handles Next.js-specific features like automatic route-based code splitting, CSS module support, and environment variable injection without additional configuration. While Turbopack handles development builds, Next.js still uses a separate production build pipeline, though Turbopack production support has been actively developed.',
  codeExample: '// package.json scripts\\n{\\n  "scripts": {\\n    "dev": "next dev --turbopack",\\n    "build": "next build"\\n  }\\n}\\n\\n// next.config.js - no Turbopack-specific config needed\\n// It respects existing Next.js configuration',
  followUp: 'Can Turbopack be used outside of Next.js as a standalone bundler?',
  keyTakeaway: 'Turbopack is the default Next.js development bundler that natively understands Next.js conventions like App Router and Server Components.',
},
{
  id: 87,
  title: 'How does Turbopack\'s incremental computation model work?',
  difficulty: 'medium',
  topic: 'turbopack',
  subtopic: 'incremental-computation',
  answer: 'Turbopack\'s incremental computation model is based on a fine-grained dependency tracking system where every function call and its inputs are tracked in a computation graph. When a file changes, Turbopack identifies exactly which computations depended on that file and invalidates only those specific nodes in the graph, recomputing the minimum necessary work. This is fundamentally different from Webpack\'s approach which often reprocesses entire module subgraphs on change. The computation graph persists across rebuilds and even across dev server restarts through disk caching. Each node in the graph stores its result along with metadata about what inputs produced that result, enabling precise cache invalidation. This granular tracking means that even in a project with thousands of modules, a single file change triggers recomputation of only the directly affected transformations and bundle updates.',
  followUp: 'How does Turbopack\'s incremental model handle changes that affect many downstream modules, such as modifying a shared utility?',
  keyTakeaway: 'Turbopack tracks dependencies at the individual function level, invalidating and recomputing only the exact computations affected by a file change.',
},
{
  id: 88,
  title: 'What is the difference between Turbopack and Webpack from an architecture perspective?',
  difficulty: 'medium',
  topic: 'turbopack',
  subtopic: 'webpack-comparison',
  answer: 'Webpack is written in JavaScript and processes modules through a linear pipeline of loaders and plugins using a single-threaded event loop, while Turbopack is written in Rust and uses a parallel, incremental computation engine. Webpack builds a complete module graph and processes the entire dependency tree on each rebuild, relying on caching plugins to speed up subsequent builds. Turbopack instead builds a computation graph where each transformation is a tracked function call that can be individually cached and invalidated. Webpack\'s plugin system uses a hook-based architecture with synchronous and asynchronous taps, while Turbopack uses a declarative rule-based system. At scale, Webpack\'s architecture leads to linear or worse time complexity for rebuilds proportional to project size, while Turbopack\'s incremental model keeps rebuild times proportional to the size of the change. Turbopack also performs bundling lazily, only processing modules that are actually requested by the browser during development.',
  followUp: 'Can Webpack plugins and loaders be used directly in Turbopack?',
  keyTakeaway: 'Webpack processes modules linearly in JavaScript with full-graph rebuilds, while Turbopack uses Rust-based parallel incremental computation that scales with change size rather than project size.',
},
{
  id: 89,
  title: 'How does Turbopack handle HMR differently from Webpack?',
  difficulty: 'medium',
  topic: 'turbopack',
  subtopic: 'hmr',
  answer: 'Turbopack\'s HMR is built on its incremental computation engine, which means it can precisely determine the minimum set of modules that need to be updated when a file changes. Webpack\'s HMR reprocesses the changed module through its entire loader chain and then propagates updates up the module tree until it finds an accept boundary, which can involve reprocessing many unchanged modules. Turbopack tracks fine-grained dependencies at the function level, so it only recomputes the exact transformations affected by the change and sends a minimal update to the browser. The Rust-based architecture also means that HMR processing itself is significantly faster because parsing, transformation, and bundling happen in native code with true parallelism. In benchmarks on large applications, Turbopack\'s HMR updates are often delivered in under 200ms regardless of project size, while Webpack\'s HMR time tends to grow linearly with codebase size.',
  followUp: 'How does Turbopack\'s HMR handle React Server Components that need to re-render on the server?',
  keyTakeaway: 'Turbopack\'s HMR leverages fine-grained incremental computation to send minimal updates in constant time, while Webpack\'s HMR time grows with project size.',
},
{
  id: 90,
  title: 'What is Turbopack\'s function-level caching?',
  difficulty: 'medium',
  topic: 'turbopack',
  subtopic: 'function-caching',
  answer: 'Turbopack\'s function-level caching means that every computation in the build pipeline is modeled as a pure function whose result is cached based on its inputs. When a function is called with inputs it has seen before, the cached result is returned immediately without re-execution. This granularity is much finer than file-level or module-level cachingindividual operations like parsing a single file, resolving an import path, or transforming a code block each have their own cache entries. The cache is content-addressed, meaning two different files with identical content produce the same cache key for identical operations. Cache entries are persisted to disk, allowing them to survive dev server restarts and even be shared across branches if the underlying file content matches. This approach means that switching between git branches only invalidates cache entries for files that actually differ between branches.',
  followUp: 'How does function-level caching handle side effects in build plugins that depend on external state?',
  keyTakeaway: 'Function-level caching stores the result of every individual build operation keyed by its inputs, enabling sub-millisecond cache hits for any unchanged computation.',
},
{
  id: 91,
  title: 'How does Turbopack\'s Turbo engine enable lazy compilation at scale?',
  difficulty: 'hard',
  topic: 'turbopack',
  subtopic: 'turbo-engine',
  answer: 'The Turbo engine is the core computation framework underlying Turbopack that implements demand-driven incremental computation. Instead of eagerly processing the entire module graph upfront, the Turbo engine only compiles modules when they are actually requested by the browser during development. When a browser requests a page, the engine traces the dependency graph from that entry point and compiles only the reachable modules, deferring compilation of modules on unvisited routes. The engine maintains a reactive dependency graph where each computation cell tracks its dependencies and dependents, enabling automatic propagation of invalidations when source files change. This reactive model is inspired by incremental computation frameworks like Salsa (used in rust-analyzer) and enables Turbopack to start serving pages almost instantly even in monorepos with hundreds of thousands of modules. The Turbo engine also supports parallel execution of independent computations across Rust\'s async runtime, utilizing all available CPU cores without the overhead of JavaScript worker threads.',
  followUp: 'How does the Turbo engine handle the transition from lazy development compilation to eager production bundling?',
  keyTakeaway: 'The Turbo engine uses demand-driven reactive computation to compile only browser-requested modules lazily, enabling near-instant startup in massive codebases.',
},
{
  id: 92,
  title: 'What are the current limitations of Turbopack and when should you still use Webpack?',
  difficulty: 'hard',
  topic: 'turbopack',
  subtopic: 'webpack-comparison',
  answer: 'Turbopack\'s primary limitation is its tight coupling to the Next.js ecosystemit cannot be used as a standalone general-purpose bundler for non-Next.js projects. The Webpack plugin and loader ecosystem is not compatible with Turbopack, so projects relying on specialized Webpack plugins for things like federation, custom asset handling, or framework-specific integrations cannot migrate directly. Turbopack\'s production build support has been under active development but historically lagged behind its dev server capabilities, meaning some teams use Turbopack for development but fall back to Webpack for production builds. Projects requiring Module Federation for micro-frontend architectures must use Webpack since Turbopack does not support this feature. Custom loader chains that perform project-specific transformations need to be reimplemented using Turbopack\'s rule system. You should continue using Webpack when you need the full breadth of its plugin ecosystem, framework-agnostic bundling, or mature production optimization features.',
  followUp: 'What is Vercel\'s roadmap for making Turbopack usable outside of the Next.js ecosystem?',
  keyTakeaway: 'Turbopack is currently limited to the Next.js ecosystem and lacks Webpack\'s plugin compatibility, Module Federation support, and framework-agnostic usage.',
},
{
  id: 93,
  title: 'What is Rspack and why was it created?',
  difficulty: 'easy',
  topic: 'rspack',
  subtopic: 'webpack-compatibility',
  answer: 'Rspack is a high-performance JavaScript bundler written in Rust that is designed to be a drop-in replacement for Webpack. It was created by ByteDance to solve the extremely slow build times they experienced with Webpack in their large-scale internal applications with millions of lines of code. Rspack aims to provide the same configuration API and plugin interface as Webpack so that existing Webpack projects can migrate with minimal changes. By reimplementing Webpack\'s core bundling logic in Rust with native parallelism, Rspack achieves 5-10x faster builds compared to Webpack while maintaining compatibility with most of the Webpack ecosystem. The project is open source and backed by ByteDance\'s infrastructure team who use it in production across hundreds of internal projects.',
  followUp: 'How does Rspack\'s approach of Webpack compatibility differ from Turbopack\'s approach of building a new API?',
  keyTakeaway: 'Rspack is a Rust-based, Webpack-compatible bundler created by ByteDance to deliver 5-10x faster builds while preserving the existing Webpack configuration API.',
},
{
  id: 94,
  title: 'How compatible is Rspack with Webpack\'s loader and plugin API?',
  difficulty: 'medium',
  topic: 'rspack',
  subtopic: 'loader-support',
  answer: 'Rspack supports the vast majority of Webpack\'s loader API, allowing most existing loaders like babel-loader, sass-loader, css-loader, and file-loader to work without modification. For common transformations like JavaScript/TypeScript compilation and CSS processing, Rspack provides built-in Rust-native implementations (using SWC) that are significantly faster than their JavaScript loader equivalents. The plugin API compatibility covers the most commonly used Webpack hooks including compilation, module, and chunk-related hooks. However, some advanced plugin hooks that depend on Webpack\'s internal implementation details may not be available or may behave slightly differently. Rspack provides its own plugin types through @rspack/core that mirror Webpack\'s plugin interface. Popular plugins like HtmlWebpackPlugin, MiniCssExtractPlugin, and DefinePlugin have Rspack-native equivalents that offer the same configuration with better performance.',
  codeExample: '// rspack.config.js - nearly identical to webpack.config.js\\nconst { HtmlRspackPlugin } = require(\'@rspack/core\')\\n\\nmodule.exports = {\\n  entry: \'./src/index.tsx\',\\n  module: {\\n    rules: [\\n      {\\n        test: /\\.tsx?$/,\\n        use: {\\n          loader: \'builtin:swc-loader\',\\n          options: { jsc: { parser: { syntax: \'typescript\', tsx: true } } }\\n        }\\n      },\\n      // existing webpack loaders also work\\n      { test: /\\.svg$/, use: \'@svgr/webpack\' }\\n    ]\\n  },\\n  plugins: [new HtmlRspackPlugin({ template: \'./index.html\' })]\\n}',
  followUp: 'What strategies does Rspack use to run JavaScript loaders efficiently from its Rust core?',
  keyTakeaway: 'Rspack supports most Webpack loaders directly and provides faster Rust-native built-in replacements for common loaders like babel-loader and css-loader.',
},
{
  id: 95,
  title: 'What are the key architectural differences between Rspack and Webpack?',
  difficulty: 'medium',
  topic: 'rspack',
  subtopic: 'rust-architecture',
  answer: 'The most fundamental difference is that Rspack\'s core is written in Rust while Webpack\'s core is JavaScript, enabling Rspack to leverage native multi-threading for parallel module processing. Rspack uses a multi-threaded module graph construction phase where parsing, transformation, and dependency resolution happen concurrently across all CPU cores, while Webpack processes modules sequentially on a single thread. Rspack implements common transformations (JS/TS compilation via SWC, CSS processing) as built-in Rust modules rather than external JavaScript loaders, eliminating the serialization overhead of passing data between the bundler core and loader processes. The code generation and optimization phases in Rspack are also parallelized in Rust, whereas Webpack performs these steps sequentially. Despite these architectural differences, Rspack maintains a JavaScript API layer that translates Webpack-compatible configuration into its Rust internals, preserving the developer experience while fundamentally changing the execution model.',
  followUp: 'How does Rspack handle the boundary between its Rust core and JavaScript plugins without excessive serialization overhead?',
  keyTakeaway: 'Rspack replaces Webpack\'s single-threaded JavaScript core with a multi-threaded Rust engine while maintaining a compatible JavaScript configuration layer.',
},
{
  id: 96,
  title: 'How do you migrate a Webpack project to Rspack?',
  difficulty: 'medium',
  topic: 'rspack',
  subtopic: 'migration',
  answer: 'Migration from Webpack to Rspack typically involves replacing the webpack dependency with @rspack/core and renaming webpack.config.js to rspack.config.js, though Rspack can also read webpack.config.js directly. The most impactful change is replacing JavaScript-based loaders with Rspack\'s built-in equivalentsfor example, replacing babel-loader or ts-loader with builtin:swc-loader for TypeScript and JSX transformation. Webpack plugins need to be replaced with their Rspack counterparts: HtmlWebpackPlugin becomes HtmlRspackPlugin, MiniCssExtractPlugin becomes CssExtractRspackPlugin. For loaders without Rspack built-in equivalents, the original Webpack loaders typically work without changes since Rspack supports the Webpack loader API. The Rspack team provides a migration guide and a compatibility table documenting which Webpack features are fully supported, partially supported, or not yet available.',
  codeExample: '// Step 1: Install\\n// npm remove webpack webpack-cli\\n// npm add @rspack/core @rspack/cli\\n\\n// Step 2: Update config (rspack.config.js)\\nconst { HtmlRspackPlugin, CssExtractRspackPlugin } = require(\'@rspack/core\')\\n\\nmodule.exports = {\\n  // Most webpack config works as-is\\n  entry: \'./src/index.tsx\',\\n  module: {\\n    rules: [{\\n      test: /\\.tsx?$/,\\n      // Replace babel-loader with builtin:swc-loader\\n      use: { loader: \'builtin:swc-loader\' }\\n    }]\\n  },\\n  plugins: [\\n    new HtmlRspackPlugin(),      // was HtmlWebpackPlugin\\n    new CssExtractRspackPlugin()  // was MiniCssExtractPlugin\\n  ]\\n}',
  followUp: 'What are the most common breaking changes teams encounter when migrating large Webpack projects to Rspack?',
  keyTakeaway: 'Migrating to Rspack involves replacing webpack with @rspack/core, swapping JS loaders for built-in Rust equivalents, and using Rspack-native plugin counterparts.',
},
{
  id: 97,
  title: 'What Webpack features are not fully supported in Rspack and why?',
  difficulty: 'hard',
  topic: 'rspack',
  subtopic: 'plugin-support',
  answer: 'Rspack does not fully support all of Webpack\'s compiler and compilation hooks, particularly the more obscure hooks that expose deep internals like normalModuleFactory, contextModuleFactory, and certain optimization-phase hooks. Module Federation, while supported in Rspack, may have behavioral differences in edge cases because Rspack\'s chunk graph construction follows a different internal algorithm optimized for parallelism. Some Webpack loader API features like loader.addContextDependency or pitch loaders with complex data passing between pitched loaders have incomplete support. Webpack\'s persistent caching (cache.type: "filesystem") works differently in Rspack because the Rust core uses its own caching mechanism rather than Webpack\'s serialization-based approach. The reason these gaps exist is architectural: Rspack\'s Rust core processes modules in a fundamentally different order and with different data structures than Webpack, making it impractical to replicate every internal behavior. Plugins that depend on the specific ordering of Webpack\'s internal processing phases or that monkey-patch Webpack internals will not work in Rspack.',
  followUp: 'How does the Rspack team prioritize which Webpack compatibility gaps to close?',
  keyTakeaway: 'Rspack lacks full support for obscure Webpack hooks, some pitch loader behaviors, and plugins that depend on Webpack\'s specific internal processing order due to fundamental architectural differences.',
},
{
  id: 98,
  title: 'What is Rolldown and what is its relationship to Vite?',
  difficulty: 'easy',
  topic: 'rolldown',
  subtopic: 'vite-integration',
  answer: 'Rolldown is a Rust-based JavaScript bundler designed to eventually replace both Rollup and esbuild in the Vite toolchain. Currently, Vite uses esbuild for development dependency pre-bundling and Rollup for production builds, which can cause subtle behavioral differences between development and production. Rolldown aims to unify these two roles into a single, fast bundler that provides consistent behavior across both environments. It is being developed by the Vite core team, with Evan You leading the project, and is designed to be a drop-in replacement for Rollup with compatible plugin API. By being written in Rust, Rolldown delivers the speed benefits of esbuild while maintaining the rich plugin ecosystem and output quality of Rollup.',
  followUp: 'Why does using two different bundlers (esbuild and Rollup) in Vite cause problems that Rolldown aims to solve?',
  keyTakeaway: 'Rolldown is a Rust-based bundler by the Vite team that aims to replace both esbuild and Rollup in Vite to unify development and production bundling.',
},
{
  id: 99,
  title: 'How does Rolldown aim to maintain compatibility with Rollup\'s plugin API?',
  difficulty: 'medium',
  topic: 'rolldown',
  subtopic: 'rollup-compatibility',
  answer: 'Rolldown implements the same plugin hook interface as Rollup, including core hooks like resolveId, load, transform, renderChunk, and generateBundle. The plugin hooks are executed in the same order and with the same semantics as Rollup, meaning existing Rollup plugins can theoretically work with Rolldown without modification. Rolldown exposes a JavaScript API layer that accepts Rollup-format plugin objects and translates hook invocations between the Rust bundling core and the JavaScript plugin code. For performance-critical plugins, Rolldown also supports native Rust plugins that bypass the JavaScript boundary entirely. The compatibility goal extends to Rollup\'s output options including format types (esm, cjs, iife, umd), chunk naming patterns, and manualChunks configuration. Where behavioral differences exist, the Rolldown team documents them explicitly and aims to minimize divergence from Rollup\'s established behavior.',
  codeExample: '// Existing Rollup plugins work in Rolldown\\nimport resolve from \'@rollup/plugin-node-resolve\'\\nimport commonjs from \'@rollup/plugin-commonjs\'\\n\\n// rolldown.config.js\\nexport default {\\n  input: \'./src/index.ts\',\\n  output: { format: \'esm\', dir: \'dist\' },\\n  plugins: [\\n    resolve(),\\n    commonjs(),\\n    // Custom plugin uses same Rollup hook API\\n    {\\n      name: \'my-plugin\',\\n      transform(code, id) {\\n        if (id.endsWith(\'.svg\')) {\\n          return { code: `export default ${JSON.stringify(code)}` }\\n        }\\n      }\\n    }\\n  ]\\n}',
  followUp: 'What performance overhead does the JavaScript-to-Rust bridge introduce for Rollup plugin compatibility?',
  keyTakeaway: 'Rolldown implements the same plugin hook interface and execution order as Rollup, allowing existing Rollup plugins to work while providing an optional native Rust plugin path for performance.',
},
{
  id: 100,
  title: 'What architectural decisions make Rolldown faster than Rollup while maintaining API compatibility?',
  difficulty: 'hard',
  topic: 'rolldown',
  subtopic: 'rust-architecture',
  answer: 'Rolldown achieves its speed advantage primarily through Rust\'s native multi-threading for parallel module parsing, transformation, and code generation, whereas Rollup processes modules sequentially in JavaScript. The module resolution and dependency graph construction phase in Rolldown runs entirely in Rust with parallel file system access and concurrent AST parsing using Oxc (a Rust-based JavaScript parser that is significantly faster than Acorn used by Rollup). Rolldown batches JavaScript plugin hook calls to minimize the overhead of crossing the Rust-JavaScript boundaryinstead of invoking a JavaScript transform hook for each module individually, it can queue multiple modules and invoke the hooks in optimized batches. The code generation phase leverages Rust\'s zero-cost abstractions and efficient string handling to produce output chunks faster than Rollup\'s JavaScript-based code generation. Rolldown also uses a more efficient internal representation for the module graph that reduces memory allocations and enables cache-friendly traversal patterns. Despite these architectural differences, the external API surface maintains compatibility by translating between Rolldown\'s optimized internal structures and Rollup\'s expected plugin hook signatures at the boundary layer.',
  followUp: 'How does Rolldown\'s use of the Oxc parser compare to Rollup\'s Acorn parser in terms of AST compatibility and parsing edge cases?',
  keyTakeaway: 'Rolldown achieves speed through parallel Rust-native parsing with Oxc, batched JavaScript plugin calls, and cache-friendly memory layouts while translating to Rollup\'s API at the boundary.',
},
]

export function filterBundlerQuestions(
  questions: BundlerInterviewQuestion[],
  difficulty: 'all' | 'easy' | 'medium' | 'hard',
  topic: string,
): BundlerInterviewQuestion[] {
  return questions.filter((q) => {
    if (difficulty !== 'all' && q.difficulty !== difficulty) return false
    if (topic !== 'all' && q.topic !== topic) return false
    return true
  })
}
